{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from split import *\n",
    "from score import *\n",
    "from scipy import interpolate\n",
    "import time \n",
    "%matplotlib inline \n",
    "import warnings \n",
    "warnings.simplefilter('ignore', np.RankWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "\"\"\"\n",
    "Title:       Search_By_Triplet + TIME_RESOLUTION + MODIFICATIONS INTRODUCED BY MIKHAEL MIKASINKO. (THE SEARCH OF TRIPLETS WILL BE DIFFERENT\n",
    "Status:      ...  \n",
    "\n",
    "Autor:       Piter Amador Paye Mamani.\n",
    "\n",
    "Description:\n",
    "             This python-code is an implementation of the algorithm Search By Triplet that was inspired on the work of\n",
    "             Daniel Campora.           \n",
    "Changes:\n",
    "        * Implementing the z cut. [doing]\n",
    "        * Change np.arctan() by np.arctan2()\n",
    "\n",
    "1.  Squeletum of the algorithm\n",
    "2.  First Tracks. \n",
    "3. \n",
    "4. \n",
    "5.  Adding exceptions. \n",
    "6.  I've deleted unnecessary comments. Also, I was getting an error at time to compute findcandidatewindows.\n",
    "    Problems. One get the values of tracks \n",
    "7.  Changing the jerarquy of the function, according to the paper. It means that findcandidatewindows is calculated\n",
    "    on all modules befero they were processed.\n",
    "6. \n",
    "7. \n",
    "8.  I've added the information of weak_tracks and I've added the information of USED and NOt USED \n",
    "9.  dphi  is a constant value\n",
    "9.  Adding timing information \n",
    "\n",
    "\n",
    "10. In this version I will plot a graphic of efficiency in function of dphi. \n",
    "    Here, I am not concentrating on the plots of the tracks. Only on the plots of the efficiency that depend on dphi.\n",
    "    In other words. I have to run the main program and get the values of the efficiency and then plot. \n",
    "    I am thinking on work only with 0.004 percent of the data. Because it is more fast than all data. \n",
    "    \n",
    "11. Adding TIMING TO THE ALGORITHM.     \n",
    "    To add timing information to the algorithm \n",
    "    I've do it previoues analysis like see if the time difference follow a gaussian distribution.\n",
    "    Plus another important question is to see. How we can add the new restriction to our values. \n",
    "    \n",
    "    I refer that I have to compute the difference in time between t1 and t2 to see if the values surpasses a new technological approach. \n",
    "12. The timing information was added to this algorithm. However, the serach by triplet algorithm is falling according to (probably) \n",
    "    the lost of one variable. It means that when we are using phi, it depend on the x and y. However, the plots that I am getting are good \n",
    "    only for a xy superposed planes. In other words, the window variable on phi is a good candidate but it lost informatin on Z X and Z Y. \n",
    "    \n",
    "=============================================\n",
    "To implement : \n",
    "\n",
    "def search_by_triplet (..., time=True):\n",
    "      ...\n",
    "      if time:  # on all cases where be necessary\n",
    "          # implement timing information here \n",
    "=============================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rho(x,y):\n",
    "    return np.sqrt(x*x + y*y)\n",
    "def r(x,y,z):\n",
    "    return np.sqrt(x*x + y*y + z*z)\n",
    "def theta(x,y,z):\n",
    "    return np.arccos(z/r(x,y,z))\n",
    "def phi(x,y):\n",
    "    return np.arctan2(y/x)\n",
    "def module(r):\n",
    "    return np.sqrt(np.sum(r*r))\n",
    "def r_e(z, r_l, r_c):\n",
    "    z_c = r_c[2] \n",
    "    r_versor = (r_l - r_c)/module(r_l - r_c)               # computing r_versor\n",
    "    r_versor_dot_z_versor = r_versor[2]  \n",
    "    return r_c - r_versor/r_versor_dot_z_versor*(z_c - z)  # IMPORTANT WITH THE MINUS SIGN.\n",
    "def correct_time(hit_time, x, y, z):\n",
    "    c = 0.299792 #[cm/ps] Light Velocity in [centimeters & pico seconds]\n",
    "    travel_time = np.sqrt(x*x + y*y + z*z)/c\n",
    "    return hit_time - travel_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtheta(x, dx, y, dy):\n",
    "    # definition of theta = arctan(x,y)\n",
    "    return np.arctan2(- x*dy + dx*y) / y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_theta():\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_time():\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_z():\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_x():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_y():\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_data(fraction, event):\n",
    "    global time_resolution           # Adding a time resolution to our analysis of tracks\n",
    "    \"\"\"\n",
    "    EVENT\n",
    "    55microns50psInner55microns50psOuter_EventNumber.txt\n",
    "    \n",
    "    25microns0psInner200microns50psOuter_test.txt\n",
    "    25microns0psInner200microns50psOuter_train.txt\n",
    "    25microns75psInner25microns75psOuter_test.txt\n",
    "    25microns75psInner25microns75psOuter_train.txt\n",
    "    55microns0psInner55microns0psOuter_test.txt\n",
    "    55microns0psInner55microns0psOuter_train.txt\n",
    "    55microns100psInner200microns50psOuter_test.txt\n",
    "    55microns100psInner200microns50psOuter_train.txt\n",
    "    55microns50psInner55microns50psOuter_test.txt\n",
    "    55microns50psInner55microns50psOuter_train.txt\n",
    "    \n",
    "    RAMPData25microns0psInner200microns50psOuter_test.txt\n",
    "    RAMPData25microns0psInner200microns50psOuter_train.txt\n",
    "    RAMPData25microns75psInner25microns75psOuter_test.txt\n",
    "    RAMPData25microns75psInner25microns75psOuter_train.txt\n",
    "    RAMPData55microns0psInner55microns0psOuter_test.txt\n",
    "    RAMPData55microns0psInner55microns0psOuter_train.txt\n",
    "    RAMPData55microns100psInner200microns50psOuter_test.txt\n",
    "    RAMPData55microns100psInner200microns50psOuter_train.txt\n",
    "    RAMPData55microns50psInner200microns50psOuter_test.txt\n",
    "    RAMPData55microns50psInner200microns50psOuter_train.txt\n",
    "    RAMPData55microns50psInner55microns50psOuter_test.txt\n",
    "    RAMPData55microns50psInner55microns50psOuter_train.txt\n",
    "    RAMPsmeared55microns200psInner55microns50psOuter_test.txt\n",
    "    RAMPsmeared55microns200psInner55microns50psOuter_train.txt\n",
    "    test.txt\n",
    "    testTrain.txt\n",
    "    \"\"\"\n",
    "    name = 'data2/55microns50psInner55microns50psOuter_EventNumber.txt' # To be modified for others files. \n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df = pd.read_csv(name, sep=' ')              # All data.\n",
    "    \n",
    "    columns = df.columns.values\n",
    "    columns[9] = 'Event'\n",
    "    df.columns = columns\n",
    "\n",
    "    df_tmp = df.query(f'Event == {event}' ) #.copy(deep = True)  # inplace=True)\n",
    "\n",
    "    \n",
    "    df_tmp2, _ = split_frac(df_tmp, fraction)\n",
    "    \n",
    "    return df_tmp2        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortbyphi():\n",
    "    global time_resolution               # Adding a time resolution to our analysis of tracks\n",
    "    \n",
    "    global df \n",
    "    # IMPORTANT, the dataframe will be modified with the values of the z_CORRECT.\n",
    "    '''Description:\n",
    "    Sort each D_i increasingly accoording to phi\n",
    "    And add a column to the dataframe_module with the name of used to accept or neglect hits. \n",
    "    '''\n",
    "    global sigma_z \n",
    "\n",
    "    # Creating new variables to \n",
    "    df['phi']   = np.arctan2(df['x'], df['y'])                        \n",
    "    df['t_c']   = correct_time(df['t'], df['x'], df['y'], df['z'])        \n",
    "    df['used']  = False                                             \n",
    "    #print(\"Index \", df.columns)\n",
    "    \n",
    "    modules = []\n",
    "    z_modules = [-277.0, -252.0, -227.0, -202.0, -132.0, -62.0, -37.0, -12.0, 13.0, 38.0, 63.0, 88.0, 113.0, 138.0, 163.0, 188.0, 213.0, 238.0, 263.0, 325.0, 402.0, 497.0, 616.0, 661.0, 706.0, 751.0]\n",
    "    \n",
    "    for z_m in z_modules:  \n",
    "        #print(sigma_z)\n",
    "        mod = df.query(f\" {z_m} - {sigma_z} <= z <= {z_m} + {sigma_z}\").copy(deep=True)\n",
    "        mod['z_mod'] = z_m\n",
    "        #mod.loc[mod.index.values, \"z_mod\"] = z_m\n",
    "        #mod.loc[mod.index.values, \"used\"]  = False\n",
    "        # IMPORTANT \n",
    "        mod = mod.sort_values('phi', ascending=True)\n",
    "        #print(\"Index \", mod['t_c'])\n",
    "        modules.append(mod)  \n",
    "    tmp_df =pd.DataFrame()\n",
    "    for mod in modules:\n",
    "        tmp_df = pd.concat([tmp_df, mod])\n",
    "    df = tmp_df\n",
    "    #print(df['t_c'])\n",
    "    return modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PHI = []\n",
    "def findcandidatewindows(left_mod, mod, right_mod ):\n",
    "    #global left_mod, mod, right_mod\n",
    "    #(left_mod, mod, right_mod ):\n",
    "    global time_resolution               # Adding a time resolution to our analysis of tracks\n",
    "    # phi_window     =  phi_extrapolation_base + np.abs( hit_Zs[h_center]) * phi_extrapolation_coef \n",
    "    global phi_extrapolation_coef, phi_extrapolation_base , dphi\n",
    "    '''Description: \n",
    "        Compute the first and last candidates(the window) according to acceptance range(dphi) for each hit.\n",
    "        SUPPOSSING THAT ALL DATA ARE ORDERED ACCOURDING TO PHI. THIS PROCCESS WAS DONE Previously\n",
    "        In case of add more information to the modules, one easily can add throught the iteration \n",
    "    '''\n",
    "    # CONVENTION :     \n",
    "    # l_m  m  r_m   the values are ordered.      \n",
    "    #  |   |   |             \n",
    "    #  |   |   |    phi up  \n",
    "    #  |   |   |    phi      \n",
    "    #  |   |   |    phi down \n",
    "    #  |   |   |          \n",
    "    \n",
    "    right_hit_max = [] \n",
    "    right_hit_min = [] \n",
    "\n",
    "    temporal = mod['phi'] \n",
    "    \n",
    "    # ITERATION OVER PHI FOR RIGHT \n",
    "    \n",
    "    for phi_i in mod['phi']: \n",
    "        #print(\"=\")\n",
    "        #print(phi_i)\n",
    "        if str(phi_i) == 'nan' :     \n",
    "            #print(phi_i, \"the value of phi_i is NaN ON RIGHT\")\n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            right_hit_min.append(m) \n",
    "            right_hit_max.append(M) \n",
    "            continue # \n",
    "        if str(phi_i) == 'NaN' :     \n",
    "            #print(phi_i, \"the value of phi_i is NaN ON RIGHT\")\n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            left_hit_min.append(m) \n",
    "            left_hit_max.append(M) \n",
    "            continue # \n",
    "            \n",
    "        z_center = mod['z_mod'].unique()[0]\n",
    "        #z = df.query(f\"phi=={phi_i}\")[\"z\"].values[0]\n",
    "        # GET HIT \n",
    "        \"\"\" dphi =  phi_extrapolation_base + np.abs( z_center ) * phi_extrapolation_coef \"\"\"\n",
    "\n",
    "        #PHI.append(dphi)\n",
    "        down      = phi_i - dphi \n",
    "        up        = phi_i + dphi \n",
    "        #print(down, up)\n",
    "        \n",
    "        condition = f'{down} <= phi <=  {up}'\n",
    "        tmp_df = right_mod.query(condition)\n",
    "        if not tmp_df.empty:\n",
    "            m = tmp_df['hit_id'][tmp_df.index[0]]     # minumum hit \n",
    "            M = tmp_df['hit_id'][tmp_df.index[-1]]    # maximum hit \n",
    "            right_hit_min.append(m) \n",
    "            right_hit_max.append(M) \n",
    "        elif tmp_df.empty :\n",
    "\n",
    "            m = \"nan\" #pd.np.nan                      # minumum hit \n",
    "            M = \"nan\" #pd.np.nan                      # maximum hit\n",
    "            right_hit_min.append(m)  \n",
    "            right_hit_max.append(M) \n",
    "            \n",
    "    left_hit_max = [] \n",
    "    left_hit_min = [] \n",
    "    # ITERATION OVER PHI FOR LEFT\n",
    "    for phi_i in mod['phi']:\n",
    "        if str(phi_i) == 'NaN' :     \n",
    "            # print(phi_i, \"the value of phi_i is NaN ON LEFT\")\n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            left_hit_min.append(m) \n",
    "            left_hit_max.append(M) \n",
    "            continue # \n",
    "        if str(phi_i) == 'nan' :     \n",
    "            # print(phi_i, \"the value of phi_i is NaN ON left\")\n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            left_hit_min.append(m) \n",
    "            left_hit_max.append(M) \n",
    "            continue # \n",
    "        # GET HIT \n",
    "        down      = phi_i - dphi \n",
    "        up        = phi_i + dphi \n",
    "        condition = f'{down} <= phi <= {up}'\n",
    "        tmp_df = left_mod.query(condition)\n",
    "        #print(\"len LEFT\", len(tmp_df))\n",
    "        if not tmp_df.empty :\n",
    "            m = tmp_df['hit_id'][tmp_df.index[0]]        # minumum hit \n",
    "            M = tmp_df['hit_id'][tmp_df.index[-1]]       # maximum hit  \n",
    "            left_hit_min.append(m)\n",
    "            left_hit_max.append(M)\n",
    "        elif tmp_df.empty :\n",
    "            # print(\"data_frame is empty LEFT\")\n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            left_hit_min.append(m) \n",
    "            left_hit_max.append(M) \n",
    "            \n",
    "    mod[\"right_hit_max\"] = right_hit_max  \n",
    "    mod[\"right_hit_min\"] = right_hit_min  \n",
    "    mod[\"left_hit_max\"]  = left_hit_max   \n",
    "    mod[\"left_hit_min\"]  = left_hit_min                                                                                    \n",
    "    return mod\n",
    "\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolation_on_center_module(r_right, r_left, z_center):\n",
    "    # IMPORTANT\n",
    "    # the values then have the next form:\n",
    "    # np(r_right), np(r_left), float(r_left)\n",
    "    #modules |  |  |\n",
    "    #        l  c  r\n",
    "    r_versor = (r_right - r_left)/module(r_right - r_left)   #  \n",
    "    distance = abs( z_center - r_left[2] )                   # \n",
    "    r_center = r_left + distance / ( np.dot(r_versor, np.array([0,0,1]) ) ) * r_versor\n",
    "    return r_center "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolation_to_origin(r1, r2, y):   # only works for y\n",
    "                                          # in case to extrapolate for x you only need to change the values of r2 ->y2 and the unitary versor\n",
    "    r_versor = (r1 - r2)/module(r1 - r2)  \n",
    "    y2       = r2[1] \n",
    "    r_origin = r2 + (y2 - y)/( np.dot(r_versor, np.array([0,-1,0]) ) ) * r_versor \n",
    "    return r_origin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = np.array( [ 2,  1,  1] )       \n",
    "r2 = np.array( [10, 110, 10] )       \n",
    "extrapolation_to_origin(r1, r2, 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_L = []\n",
    "T_R = []\n",
    "T_C = []\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "Z = []\n",
    "\n",
    "def trackseeding():\n",
    "    global sigma_z_origin\n",
    "    global dx, dy\n",
    "    global M_i\n",
    "    global time_resolution                             # Adding a time resolution to our analysis of tracks\n",
    "\n",
    "    global left_mod, mod, right_mod, M_i, dphi, sigma_t\n",
    "    '''\n",
    "    Description: \n",
    "        Checks the preceding and following modules for compatible hits using the above results.\n",
    "        \n",
    "        All triplets in the search window are fitted and compared.\n",
    "        \n",
    "        and the best one is kept as a track seed.\n",
    "        \n",
    "        stores its best found triplet\n",
    "        Finding triplets is ap- plied in first instance to the modules\n",
    "        that are further apart from the collision point\n",
    "        Each triplet is the seed of a forming track\n",
    "    '''\n",
    "    #Necessary functions.\n",
    "    def fit(triplet): \n",
    "        phi_data = [ df.query(f'hit_id == {hit}')['phi']     for hit in triplet ]\n",
    "        z_data   = [ df.query(f'hit_id == {hit}')['z_mod']   for hit in triplet ]\n",
    "        phi_data = [ hit.values[0] for hit in phi_data                      ]                        \n",
    "        z_data   = [ hit.values[0] for hit in z_data                        ]                    \n",
    "        # Kind of fit: Linear\n",
    "        fitting = np.polyfit(phi_data, z_data, 1)\n",
    "        chiSquared = np.sum((np.polyval(fitting, z_data) - phi_data)**2)\n",
    "        return chiSquared\n",
    "\n",
    "    df_triplets = []\n",
    "    #print(\"error ??????\", mod )\n",
    "    # print(\"error_mod.columns:\", mod.columns)\n",
    "    for index, part in mod.iterrows():\n",
    "\n",
    "        r_hit_max, r_hit_min = part[\"right_hit_max\"], part[\"right_hit_min\"]  \n",
    "        l_hit_max, l_hit_min = part[\"left_hit_max\"],  part[\"left_hit_min\" ] \n",
    "        \n",
    "        if  str(r_hit_max)  == \"nan\":\n",
    "            continue \n",
    "        elif str(r_hit_min) == \"nan\":\n",
    "            continue \n",
    "        elif str(l_hit_max) == \"nan\":\n",
    "            continue \n",
    "        elif str(l_hit_min) == \"nan\":\n",
    "            continue  \n",
    "        if  str(r_hit_max)  == \"NaN\" :\n",
    "            continue \n",
    "        elif str(r_hit_min) == \"NaN\" :\n",
    "            continue \n",
    "        elif str(l_hit_max) == \"NaN\" :\n",
    "            continue \n",
    "        elif str(l_hit_min) == \"NaN\" :\n",
    "            continue  \n",
    "        r_phi_max = right_mod.query(f\"hit_id == {r_hit_max}\")['phi'].values[0]   \n",
    "        r_phi_min = right_mod.query(f\"hit_id == {r_hit_min}\")['phi'].values[0]   \n",
    "                                                                                 \n",
    "        l_phi_max = left_mod.query(f\"hit_id == {l_hit_max}\")['phi'].values[0]   \n",
    "        l_phi_min = left_mod.query(f\"hit_id == {l_hit_min}\")['phi'].values[0]     \n",
    "        \n",
    "        \"\"\"\n",
    "        #left_mod.query(f\" {l_phi_min} <= phi <= {l_phi_max}\")\n",
    "        tmp_right = right_mod.query(f\"   {r_phi_min} <= phi <= {r_phi_max} & used == False  \")    # ADDING TIME\n",
    "        for R in tmp_right['phi'].values:\n",
    "            tmp_left = left_mod.query(f\" {l_phi_min} <= phi <= {l_phi_max} & used == False \")     # ADDING TIME\n",
    "            for L in tmp_left['phi'].values: \n",
    "        \"\"\" \n",
    "        \n",
    "        tmp_right = right_mod.query(f\"   {r_phi_min} <= phi <= {r_phi_max} & used == False  \")    # ADDING TIME\n",
    "        for hit_right in tmp_right['hit_id'].values:\n",
    "            tmp_left = left_mod.query(f\" {l_phi_min} <= phi <= {l_phi_max} & used == False \")     # ADDING TIME\n",
    "            for hit_left in tmp_left['hit_id'].values:         \n",
    "                \n",
    "                #hit_left   = int( tmp_left.query( f\" phi == {L}\")['hit_id'].values[0]  )  \n",
    "                hit_center = int( part[\"hit_id\"] )\n",
    "                #hit_right  = int( tmp_right.query(f\" phi == {R}\")['hit_id'].values[0]  )\n",
    "                \n",
    "                try :\n",
    "                    r_right  = tmp_right.query(f'hit_id == {hit_right} ')[['x', 'y','z']].to_numpy()[0]\n",
    "                    z_center = part['z_mod']                            \n",
    "                    r_left   = tmp_left.query(f'hit_id == {hit_left} ')[['x', 'y','z']].to_numpy()[0]\n",
    "                    #print(r_right)\n",
    "                    #print(z_center)\n",
    "                    #print(r_left)\n",
    "                except :\n",
    "                    print(\"here there is a problem\")\n",
    "                    return 1\n",
    "                \n",
    "                try :\n",
    "                    r_center_extrapolation = extrapolation_on_center_module(r_right, r_left, z_center)\n",
    "                    # MAKING A PROOF ON THE EXTRAPOLATION OF DATA. PLOTING THE VALUES OF X Y Z \n",
    "                    # make a plot of r_left and r_right and r_extrapolated\n",
    "                    #r_left, r_center_extrapolation, r_right \n",
    "                    \n",
    "                    x_hits = [r_left[0], r_center_extrapolation[0], r_right[0] ]\n",
    "                    y_hits = [r_left[1], r_center_extrapolation[1], r_right[1] ]\n",
    "                    z_hits = [r_left[2], r_center_extrapolation[2], r_right[2] ]\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    verification = r_left[2] < r_center_extrapolation[2] < r_right[2]\n",
    "                    if verification == False :\n",
    "                        print(\"verifing if the value of z_center is on the \")\n",
    "                        print(verification)\n",
    "                    #print(temporal, type(temporal))\n",
    "                except :    \n",
    "                    print(\"here is the error ==\")\n",
    "                    return 1 \n",
    "                ############################################################################################################ \n",
    "                ############################################################################################################ \n",
    "                ########################################   CUT on Z   ###################################################### \n",
    "                ############################################################################################################   \n",
    "                ############################################################################################################ \n",
    "                try:\n",
    "                    x0 , y0,  z0 = extrapolation_to_origin(r_right, r_left, 0) \n",
    "                    #if y0 != 0 :\n",
    "                    #    print(\"the value of y0 is :\", y0)\n",
    "                    #X.append(x0) \n",
    "                    #Y.append(y0) \n",
    "                    #Z.append(z0) \n",
    "                    if  abs(z0) > sigma_z_origin:\n",
    "                        #print(\"z0 > sigma_origin: \")\n",
    "                        continue \n",
    "                except : \n",
    "                    print(\"cut on z is the error\")\n",
    "\n",
    "                ############################################################################################################ \n",
    "                ############################################################################################################ \n",
    "                ########################################   NEW WINDOW on X and Y   ######################################### \n",
    "                ############################################################################################################ \n",
    "                ############################################################################################################\n",
    "                left_cut_x  = r_center_extrapolation[0] - dx \n",
    "                right_cut_x = r_center_extrapolation[0] + dx  \n",
    "                \n",
    "                down_cut_y  = r_center_extrapolation[1] - dy \n",
    "                up_cut_y    = r_center_extrapolation[1] + dy       \n",
    "                \n",
    "                #print(\"left_cut_x\", \"right_cut_x\", \"down_cut_y\", \"up_cut_y\")\n",
    "                #print(left_cut_x, right_cut_x, down_cut_y, up_cut_y)\n",
    "                \n",
    "                try : \n",
    "                    ############################################################################################################ \n",
    "                    ############################################################################################################ \n",
    "                    ########################################   DEEP CONDITION on X and Y   ##################################### \n",
    "                    ############################################################################################################ \n",
    "                    ############################################################################################################\n",
    "                    x = part['x']\n",
    "                    y = part['y']\n",
    "                    #print(\"verifying the kind of x and y \", x, y, type(x), type(y))\n",
    "                    \n",
    "                    new_window = mod.query(f\" {left_cut_x}  < x < {right_cut_x} & {down_cut_y} < y < {up_cut_y} \").copy(deep=True) \n",
    "                    if  (left_cut_x  < x < right_cut_x) and (down_cut_y < y < up_cut_y) :\n",
    "                        \"\"\"\n",
    "                        #if len(new_window) > 0 :\n",
    "                        print(\"PROOF\", part[['x', 'y', 'z']].to_numpy())\n",
    "                        point = part[['x', 'y', 'z']].to_numpy()\n",
    "                        plt.plot(z_hits, x_hits) \n",
    "                        plt.scatter(z_hits + [ point[2], point[2], point[2]], x_hits+[point[0], left_cut_x, right_cut_x] )\n",
    "                        plt.xlabel(\"z\")\n",
    "                        plt.ylabel(\"x\")\n",
    "                        plt.show() \n",
    "                        plt.plot(z_hits, y_hits ) \n",
    "                        plt.scatter(z_hits + [ point[2], point[2], point[2]], y_hits+[point[1], down_cut_y, up_cut_y] )\n",
    "                        plt.xlabel(\"z\")\n",
    "                        plt.ylabel(\"y\")\n",
    "                        plt.show()\n",
    "                        \"\"\"\n",
    "                        pass\n",
    "\n",
    "                    else:\n",
    "                        continue \n",
    "                except : \n",
    "                    print(\"the new window has a syntax error\")\n",
    "                    return\n",
    "                \n",
    "                #print(\"print the new window\", len(new_window)\n",
    "                \n",
    "                ############################################################################################################ \n",
    "                ############################################################################################################ \n",
    "                ########################################   TIMING   ######################################################## \n",
    "                ############################################################################################################ \n",
    "                ############################################################################################################ \n",
    "\n",
    "                # NOTATION: 't_c' is the corrected time. Against of t_c that is the time variable of the modules t_center        \n",
    "                \n",
    "                if time_resolution == True :\n",
    "                    # print(\"time_resolution == True\")\n",
    "                    t_l =   left_mod.query(f'hit_id == {hit_left}')['t_c'].values[0]\n",
    "                    t_c =      mod.query(f'hit_id == {hit_center}')['t_c'].values[0]\n",
    "                    t_r = right_mod.query(f'hit_id == {hit_right}')['t_c'].values[0]\n",
    "\n",
    "                    # CONDITIONS:\n",
    "                    T_L.append(abs(t_l - t_c))\n",
    "                    T_C.append(abs(t_c - t_r))\n",
    "                    T_R.append(abs(t_l - t_r))\n",
    "                    \n",
    "                    if abs(t_l - t_c) > 3*sigma_t :\n",
    "                        continue\n",
    "                    if abs(t_c - t_r) > 3*sigma_t :\n",
    "                        continue\n",
    "                    if abs(t_l - t_r) > 3*sigma_t :\n",
    "                        continue\n",
    "                ############################################################################################################\n",
    "                ############################################################################################################\n",
    "                ############################################################################################################\n",
    "                ############################################################################################################\n",
    "                ############################################################################################################\n",
    "                \n",
    "                # With this data we have built the triplets. \n",
    "                triplets = [hit_left, hit_center, hit_right] \n",
    "                \n",
    "                # This a lost of memory. I mean that call by hits and not by values is a lost of memory.\n",
    "                chi2 = fit(triplets)                                                                                                                                                                \n",
    "                # Finally we append the values of the data to a df_triplets\n",
    "                df_triplets.append(list(triplets)+[chi2])\n",
    "                        \n",
    "                \n",
    "                \n",
    "    df_triplets = pd.DataFrame(df_triplets, columns = ['left_hit', 'hit', 'right_hit', 'chi2'])  \n",
    "    # Up to this point it is necessary to have the values of df_triplets complete\n",
    "    # Then the algorithm should continue to get the best choices according to the values\n",
    "    # of chi2. \n",
    "    \n",
    "    def best_choice(df_triplets):\n",
    "        seeds = []\n",
    "        for hit_c in df_triplets['hit'].unique() : # UNIQUE\n",
    "            # GROUPING \n",
    "            tmp = df_triplets.query(f'hit == {hit_c}')\n",
    "            minimum = (tmp['chi2']).idxmin()\n",
    "            t = (tmp.loc[minimum]).values     \n",
    "            t = [int(i) for i in t[:3]]\n",
    "            #these are the triplets       \n",
    "            \n",
    "            seeds.append(list(t[:3]))     # Here I am negleting the information chi2 because is not important\n",
    "        return seeds                      # obviously it is a track\n",
    "    \n",
    "    seeds = best_choice(df_triplets)\n",
    "    \n",
    "    for seed in seeds:\n",
    "            # #########     MARKING TRIPLES######  \n",
    "            # MATCHING EACH HIT AS USED ON THE WORKING MODULE  \n",
    "            hit_id_left, hit_id_center, hit_id_right = seed \n",
    "            #LEFT\n",
    "            left_mod.loc[   left_mod.hit_id == hit_id_left,    \"used\" ]     = True\n",
    "            #CENTER\n",
    "            mod.loc[           mod.hit_id   == hit_id_center,  \"used\" ]     = True\n",
    "            #RIGTH\n",
    "            right_mod.loc[ right_mod.hit_id == hit_id_right,   \"used\" ]     = True\n",
    "  \n",
    "    return seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRACKS = []\n",
    "\n",
    "def track_forwarding():\n",
    "    global dx, dy\n",
    "    global frozen_tracks                 #\n",
    "    global time_resolution               # Adding a time resolution to our analysis of tracks\n",
    "    # Here I have to review the leff_mod\n",
    "    # So, I am beging to print the value of left_mod on three steps \n",
    "\n",
    "    global tracks, work_module, left_mod, mod, right_mod, M_i, weak_tracks \n",
    "    # for \n",
    "    global phi_extrapolation_coef, phi_extrapolation_base \n",
    "\n",
    "    #global TRACKS\n",
    "    \n",
    "    new_tracks = []\n",
    "    frozen_tracks = [] \n",
    "    # Notation:\n",
    "    # x0, y0, z0 is the EXTRAPOLATED track.               \n",
    "    # X,  Y,  Z  is the last track on previous module.   \n",
    "    # x,  y,  z  is the tracks on a window.                                                                 \n",
    "    # Searching tracks on phi_e - dphi < phi < phi_e + dphi that minimize the extrapolated function.\n",
    "    # r0 = np.array([x0, y0, z0] )\n",
    "    # r  = np.array([x, y, 1] )\n",
    "    # R  = np.array([X,  Y,  Z ] )\n",
    "    def module(r):\n",
    "        return np.sqrt(np.sum(r*r))\n",
    "    def ext_func(r0, r1, r):\n",
    "        # r0, r1, r are arrays\n",
    "        dx2_plus_dy2 = module(  r0-r )     # distance between hits on the working module.  \n",
    "        \"\"\"dz2       = module( r1-r0 )     # distance between the last two modules.                                \n",
    "        return dx2_plus_dy2/dz2 \n",
    "        \"\"\"  \n",
    "        return dx2_plus_dy2 \n",
    "    \n",
    "    # print(\"========================================\")\n",
    "    # print(work_module['z_mod'].unique()[0])\n",
    "    # print(\"========================================\")\n",
    "\n",
    "    \n",
    "    \n",
    "    try: \n",
    "        z_e = work_module['z_mod'].unique()[0]  # z_position of work_module  # an array  \n",
    "    except :\n",
    "        \n",
    "        print(\"possible error on work_module. Probably it not have values\" )\n",
    "        return \n",
    "    \n",
    "    # print(\"error\",  tracks)\n",
    "    # here the track is exactly the seed. Only for the 1th iteration\n",
    "    for track in tracks: \n",
    "                        \n",
    "        # print(\"error\", time_resolution)\n",
    "        #PROOF: Do you have the track values information of USED ?\n",
    "        data = []   \n",
    "        vector_data = []\n",
    "        #EXTRAPOLATION ONLY WITH TWO LAST HITS \n",
    "        for hit in track[0:2] :\n",
    "            data.append(tuple((df.query(f'hit_id == {hit}')[['phi', 'z_mod']]).values[0]))     \n",
    "            vector_data.append(tuple((df.query(f'hit_id == {hit}')[['x', 'y', 'z_mod']]).values[0]))\n",
    "        phi_data, z_data = zip(*data) \n",
    "        \n",
    "        #EXTRAPOLATED SEGMENT FUNCTION      \n",
    "        ext_seg = interpolate.interp1d(z_data, phi_data, fill_value = \"extrapolate\" )\n",
    "        phi_e   = ext_seg( z_e )                   # an array \n",
    "        r_l, r_c = vector_data                   # THE VALUES ON LEFT AND RIGHT                                                \n",
    "        r_l, r_c = np.array(r_l), np.array(r_c)  # \n",
    "        x_e, y_e, z_e = r_e(z_e, r_l, r_c)       # COMPUTING THE VALUES ON THE WORKING MODULE.                                 \n",
    "        \n",
    "        #Open a Window centered on phi_e: \n",
    "        z_center = mod['z_mod'].unique()[0]\n",
    "        #z = df.query(f\"phi=={phi_i}\")[\"z\"].values[0]\n",
    "        # GET HIT \n",
    "        \"\"\" dphi =  phi_extrapolation_base + np.abs( z_center ) * phi_extrapolation_coef \"\"\"\n",
    "        \n",
    "        down = phi_e - dphi\n",
    "        up   = phi_e + dphi   \n",
    "        \n",
    "        #print(type(down), type(up))\n",
    "                \n",
    "        down = (down)  #phi_e - dphi\n",
    "        up   = (up)    #phi_e + dphi    \n",
    "        \n",
    "        if str(down) == 'nan' or str(down) == 'NaN' or str(up) == 'nan' or str(up) == 'NaN' :\n",
    "            print(\"An error ocurred with the values of down or up. Plese cheack.\")\n",
    "            break\n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "        ######################          WINDOW        ######################### \n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "        ######################          TIMING        ######################### \n",
    "        #######################################################################\n",
    "        if time_resolution == True : \n",
    "        \n",
    "            h_l = track[0]   \n",
    "            h_c = track[1]   \n",
    "            h_r = track[2]  \n",
    "\n",
    "            t1 = df.query( f\"hit_id == {h_l}  \"   )['t_c'].values[0]   #  track      \n",
    "            t2 = df.query( f\"hit_id == {h_c}  \"   )['t_c'].values[0]   #  track       \n",
    "            t3 = df.query( f\"hit_id == {h_r}  \"   )['t_c'].values[0]   #  track          \n",
    "\n",
    "        #t4 =   # this time is computed from the dataframe\n",
    "\n",
    "        #df_work_module_window = work_module.query(f\"  abs(z_mod - 1/3.*( {t1} + {t2} + {t3} ) ) <= 3*{sigma_t} \")\n",
    "        \n",
    "        # print(\"ERROR\", time_resolution)\n",
    "        \n",
    "        if   time_resolution == True  :        # on all cases where be necessary\n",
    "                                               # on all cases where be necessary\n",
    "            df_work_module_window = work_module.query(f\"{down} <= phi <= {up}  & abs(z_mod - 1/3.*( {t1} + {t2} + {t3} ) ) <= 3*{sigma_t}\")\n",
    "            # print(\"len\", len(df_work_module_window))\n",
    "        \n",
    "        elif time_resolution == False :\n",
    "            df_work_module_window = work_module.query(f\"{down} <= phi <= {up}\")\n",
    "            #df_work_module_window = work_module.query(f\"{down} <= phi <= {up}  & abs(z_mod - 1/3.*( {t1} + {t2} + {t3} ) ) <= 3*{sigma_t}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"============\")\n",
    "        #print(type(down), type(up))     # t1, type(t1), t1, type(t1))\n",
    "        #print((down), (up))\n",
    "        #print(\"len\", len(df_work_module_window))\n",
    "        #print(\"============\")\n",
    "\n",
    "        #df_work_module_window = \n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "        ####################                            #######################\n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "        \n",
    "        #This definition will be done after the loop. \n",
    "        #df_candidates = pd.DataFrame(columns=[\"hit_id\", \"ext_fun\"])     # This dataframe have to be reviwed \n",
    "        hit_left = track[0]   \n",
    "        R  = df.query(f'hit_id == {hit_left}')[['x','y','z_mod']].values[0]  # this value would have to change on each track\n",
    "        r0 = np.array([x_e, y_e ,z_e ])\n",
    "\n",
    "            \n",
    "        \n",
    "        tmp_candidates = []\n",
    "        for index, row in df_work_module_window.iterrows(): \n",
    "            # Here I only need to have the information of position.\n",
    "            r      =  row[['x', 'y', 'z_mod']].values # this value is variable on each row.\n",
    "            hit_id =  row['hit_id']    \n",
    "            ext_func_value = ext_func(r0, R, r)\n",
    "            tmp_candidates.append( [hit_id, ext_func_value] )\n",
    "        \n",
    "        #\"************************************************************************************************************\" \n",
    "        #\"************************************If any extrapolated data is not founded on the working module **********\" \n",
    "        #\"************************************************************************************************************\"\n",
    "        #\"**********************************HERE WE ARE LOSING TRACKS*************************************************\"\n",
    "        #\"************************************************************************************************************\"\n",
    "        #\"************************************************************************************************************\"\n",
    "        \n",
    "        \n",
    "        # In case of don't find a hit on the working module.  \n",
    "        if tmp_candidates == [] :  # INEFICIENT                                            \n",
    "            # there are two cases\n",
    "            # = = \n",
    "            hit_id_left, hit_id_center, hit_id_right = track[0:3]   \n",
    "\n",
    "            #TRACKS.append(track) \n",
    " \n",
    "            # the track has its first forwarding \n",
    "            if   ( (hit_id_left in left_mod['hit_id'].values ) ): # and  (hit_id_center in mod['hit_id'].values) and (hit_id_right in right_mod['hit_id'].values ) ) : \n",
    "                same_track      = track  \n",
    "                new_tracks.append(same_track)\n",
    "                continue\n",
    "            # the track has its second worwarding     \n",
    "            elif ( (hit_id_left in mod['hit_id'].values )  ):   #and  (hit_center in right_mod['hit_id'].values) ):\n",
    "                # Add to weak_tracks    \n",
    "                if(   len(track) == 3 ) :\n",
    "                    weak_tracks.append(track)\n",
    "                    continue \n",
    "                elif( len(track) >  3 ):\n",
    "                    same_track     = track  \n",
    "                    frozen_tracks.append(same_track)\n",
    "                    continue\n",
    "                # weak_tracks.append(track)\n",
    "                #continue\n",
    "                #print(\"does the code enter here\")\n",
    "                #continue\n",
    "        # \"************************************************************************************************************\"\n",
    "        # \"************************************************************************************************************\" \n",
    "        # \"************************************tmp_candidates**********************************************************\" \n",
    "        # \"************************************************************************************************************\"\n",
    "                \n",
    "        df_candidates = pd.DataFrame(tmp_candidates, columns=[\"hit_id\", \"ext_fun\"])\n",
    "        \n",
    "        if len(tmp_candidates) == 0 : \n",
    "            print(\"an error ocurred with df_candidates\")\n",
    "            break\n",
    "        \n",
    "        # Choosing new hit_id to complete the track.  \n",
    "        new_hit_id    = df_candidates.loc[df_candidates['ext_fun'].idxmin()]['hit_id']\n",
    "        new_hit_id    = int(new_hit_id)                                           # new_hit_id   \n",
    "    \n",
    "        # MARKING EACH HIT AS USED ON THE WORKING MODULE        \n",
    "        work_module.loc[ work_module.hit_id == new_hit_id, \"used\" ]  = True   \n",
    "        \n",
    "\n",
    "        new_track     =  [new_hit_id] + track  \n",
    "        new_tracks.append(new_track)\n",
    "    \n",
    "        ####PROOF####\n",
    "        #tracks \n",
    "\n",
    "        for track in new_track : \n",
    "            proof = df.query(f\"hit_id == {track}\")['z'].tolist()\n",
    "            if sorted(proof) != proof:\n",
    "                print(\"here is the error\", track)\n",
    "\n",
    "    return new_tracks  # this value will be replaced by tracks on the main algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doing_plots(df, tracks):\n",
    "    #global weak_tracks, df, TRACKS\n",
    "    matplotlib.rc('text', usetex=True)\n",
    "    matplotlib.rcParams['text.latex.preamble'] = [r'\\boldmath']\n",
    "\n",
    "    # List will be used to create a text file \n",
    "    # Create plots to show the reconstructed tracks\n",
    "\n",
    "    #df_real_tracks = df.groupby(['particle_id'])['hit_id'].unique() \n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    #tracks = tracks.to_list() + weak_tracks \n",
    "\n",
    "    for track in tracks:\n",
    "        # Here I can get the values of the orignal dataframe.\n",
    "        #data = []\n",
    "        #for hit in track : # \n",
    "        #    data.append(list(df.query(f\"hit_id == {hit}\").values[0])) # what kind of data we want.\n",
    "        #data = pd.DataFrame(data, columns=list(df.columns.values) )\n",
    "        #print(\"dataframe: \", data[['z', 'y']])\n",
    "        \n",
    "        z = df.query(f\"hit_id == {track}\")['z'].tolist()\n",
    "        y = df.query(f\"hit_id == {track}\")['y'].tolist()\n",
    "        \n",
    "        plt.plot(z, y, '-', alpha=0.8, lw=2)\n",
    "        plt.scatter(z, y, marker='+' )\n",
    "        #print(data['hit_id'])\n",
    "        #plt.plot(data['z'], data['y'], '-', alpha=0.8, lw=2)\n",
    "        #plt.scatter(data['z'], data['y'], marker='+' )\n",
    "\n",
    "        #plt.plot(df['z'], df['y'], '-', alpha=0.8, lw=2, color='C0')\n",
    "        plt.xlabel(r\"\\textbf{Z}\")\n",
    "        plt.ylabel(r'\\textbf{Y}')\n",
    "        plt.grid(True)\n",
    "        # tracks.append(data['hit_id'])\n",
    "    plt.scatter(df['z'], df['y'], marker='+', color='b')\n",
    "    for particle_id in df.particle_id.unique() : \n",
    "        plt.plot(   df.query(f\"particle_id =={particle_id}\")['z'], df.query(f\"particle_id =={particle_id}\")['y'], '-', alpha=0.1, lw=1, color='k')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    for track in tracks:\n",
    "        # Here I can get the values of the orignal dataframe.\n",
    "        #data = []\n",
    "        #for hit in track : # \n",
    "        #     data.append(list(df.query(f\"hit_id == {hit}\").values[0])) # what kind of data we want.\n",
    "        #data = pd.DataFrame(data, columns=list(df.columns.values) )\n",
    "        #print(\"dataframe: \", data[['z', 'y']])\n",
    "        \n",
    "        z = df.query(f\"hit_id == {track}\")['z'].tolist()\n",
    "        x = df.query(f\"hit_id == {track}\")['x'].tolist()\n",
    "        \n",
    "        plt.plot(z, x, '-', alpha=0.8, lw=2)\n",
    "        plt.scatter(z, x, marker='+' )\n",
    "        #print(data['hit_id'])\n",
    "        #plt.plot(data['z'], data['y'], '-', alpha=0.8, lw=2)\n",
    "        #plt.scatter(data['z'], data['y'], marker='+' )\n",
    "        \n",
    "        #plt.plot(df['z'], df['x'], '-', alpha=0.8, lw=2, color='C0')\n",
    "        plt.xlabel(r\"\\textbf{Z}\")\n",
    "        plt.ylabel(r'\\textbf{X}')\n",
    "        plt.grid(True)\n",
    "    plt.scatter(df['z'], df['x'], marker='+', color='b' )\n",
    "    for particle_id in df.particle_id.unique() : \n",
    "        plt.plot(   df.query(f\"particle_id =={particle_id}\")['z'], df.query(f\"particle_id =={particle_id}\")['x'], '-', alpha=0.2, lw=1, color='k')\n",
    "    plt.show()    \n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    for track in tracks:\n",
    "        # Here I can get the values of the orignal dataframe.\n",
    "        #data = []\n",
    "        #for hit in track : # \n",
    "        #     data.append(list(df.query(f\"hit_id == {hit}\").values[0])) # what kind of data we want.\n",
    "        #data = pd.DataFrame(data, columns=list(df.columns.values) )\n",
    "        #print(\"dataframe: \", data[['z', 'y']])\n",
    "        \n",
    "        y = df.query(f\"hit_id == {track}\")['y'].tolist()\n",
    "        x = df.query(f\"hit_id == {track}\")['x'].tolist()\n",
    "        \n",
    "        plt.plot(x, y, '-', alpha=0.8, lw=2)\n",
    "        plt.scatter(x, y, marker='+' )\n",
    "        #print(data['hit_id'])\n",
    "        #plt.plot(data['z'], data['y'], '-', alpha=0.8, lw=2)\n",
    "        #plt.scatter(data['z'], data['y'], marker='+' )\n",
    "        plt.scatter(df['x'], df['y'], marker='+' )\n",
    "        #plt.plot(   df['x'], df['y'], '-', alpha=0.8, lw=2, color='C0')\n",
    "        plt.xlabel(r\"\\textbf{Y}\")\n",
    "        plt.ylabel(r'\\textbf{X}')\n",
    "        plt.grid(True)\n",
    "        # tracks.append(data['hit_id'])\n",
    "    \n",
    "    plt.scatter(df['x'], df['y'], marker='+', color='b')        \n",
    "    for particle_id in df.particle_id.unique() : \n",
    "        plt.plot(   df.query(f\"particle_id =={particle_id}\")['x'], df.query(f\"particle_id =={particle_id}\")['y'], '-', alpha=0.2, lw=1, color='k')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "################################## MAIN ################################################\n",
    "########################################################################################\n",
    "############################ GENERAL ALGORITHM #########################################\n",
    "############################     PARAMETERS    #########################################\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "\n",
    "# Probably I can put on this \n",
    "def search_by_triplet( TIME_RESOLUTION = True):\n",
    "    global sigma_z_origin\n",
    "    global dx, dy\n",
    "    global frozen_tracks\n",
    "    global M_i\n",
    "    global event, df, time_resolution\n",
    "\n",
    "    time_resolution = TIME_RESOLUTION \n",
    "    \n",
    "    global fraction, df\n",
    "    global modules, dphi, mod, right_mod, left_mod, sigma_t, work_module, sigma_z, tracks, weak_tracks\n",
    "    \n",
    "    ### switch time ###...\n",
    "    if   time_resolution == True:                                             # on all cases where be necessary\n",
    "        print(\"the time_resolution is activated ... \")                        # implement timing information here \n",
    "    ### switch time ###...\n",
    "    elif time_resolution == False:                                            # on all cases where be necessary\n",
    "        print(\"the time_resolution is de-activated ... \")                     # on all cases where be necessary\n",
    "    \n",
    "    T1 = time.time()  # Timing The Run-Time\n",
    "    \n",
    "    ########################################################################################\n",
    "    ###############################   PARAMETERS   #########################################\n",
    "    ######################################################################################## \n",
    "\n",
    "    dx = 0.1  # 58771/np.sqrt(12)                                                        \n",
    "    dy = 0.1  # 0.0158771/np.sqrt(12)                                                      \n",
    "    event = 2\n",
    "    sigma_t  = 1            # General Parameters\n",
    "    sigma_z  = 0.5          # General Parameters \n",
    "    sigma_z_origin = 3 * 50 # 50mm\n",
    "    fraction = 1            # General Parameters\n",
    "    dphi     = 0.01         # The windows is a variable quantity that depend on phi_ext_base ###### \n",
    "    #phi_window     =  phi_extrapolation_base + np.abs( hit_Zs[h_center]) * phi_extrapolation_coef         ;\n",
    "    #phi_extrapolation_coef = 0.02\n",
    "    #phi_extrapolation_base = 0.03                               \n",
    "    \n",
    "    m = 10    # number of modules counted from the left.  #\n",
    "    #m = 24      # from 1 to 24. No more. \n",
    "    ########################################################################################\n",
    "    ########################################################################################\n",
    "    ########################################################################################  \n",
    "    new_tracks    = []                   # where data is unmodified.   \n",
    "    frozen_tracks = []                   # these tracks are formed by more than 4 hits, it is important that it will be joined with the frozen_tracks \n",
    "    \n",
    "    df = reading_data(fraction, event)   # where data is unmodified.\n",
    "    \n",
    "    # print(\"error ¿¿¿¿¿\", len(df))\n",
    "    \n",
    "    # df_search   = df_original          # where I am searching \n",
    "    tracks = []                          # [[1,24, 5], [7,6,4] ,[346,7,32,], ... \n",
    "    weak_tracks  =[]                     # 123\n",
    "    # *********************IMPORTANT********************************************************\n",
    "    # The information of tracks is ordered. \n",
    "    # Because, each of its elements are an ordered list according to module layers.\n",
    "    # However, the information of hits are unique and not matter if are a ordered set. \n",
    "    # But it was filled out in order\n",
    "\n",
    "    # SEPARATION BY MODULE  \n",
    "    modules = sortbyphi()                # this line modify df adding the z_correct\n",
    "    # print(\"second_error\", len(modules[0]))  \n",
    "    #for i in range(len(modules)):         \n",
    "    #    print(modules[i]['t_c'])   #   \n",
    "    \n",
    "    # FIND CANDIDATE WINDOWS. In order to minimize the amount of candidates considered in subqsequent steps.\n",
    "    \n",
    "    for M_i in range(len(modules)-1-1, len(modules)-m-2, -1): \n",
    "        #M_i = M_i - 1\n",
    "        left_mod     =  modules[M_i - 1] #.copy(deep=True)      \n",
    "        mod          =  modules[M_i    ] #.copy(deep=True)   \n",
    "        right_mod    =  modules[M_i + 1] #.copy(deep=True)  \n",
    "        modules[M_i] =  findcandidatewindows(left_mod, mod, right_mod).copy(deep=True)     \n",
    "    \n",
    "    #ITERATION OVER MODULES ( ):\n",
    "\n",
    "    for M_i in range(len(modules)-1-1, len(modules)-m-2, -1) :  # the number two is due to 1. index postion default. 2. \n",
    "        # TIMING THE RUNNING OVER A MODULE\n",
    "        t1 = time.time()                \n",
    "        print(f\"module number {M_i}\")\n",
    "        #M_i = M_i - 1\n",
    "        #1th STEP:  ASIGNING NOTATION\n",
    "        left_mod  =  modules[M_i - 1]#.copy(deep=True)   \n",
    "        mod       =  modules[M_i    ]#.copy(deep=True)   \n",
    "        right_mod =  modules[M_i + 1]#.#copy(deep=True)\n",
    "        \n",
    "        #print(\"posible_error\", len(left_mod), len(mod), len(right_mod))\n",
    "        #print(\"posible_error\", len(left_mod), len(mod), len(right_mod))\n",
    "        #print(\"posible_error\", len(left_mod), len(mod), len(right_mod))\n",
    "        \n",
    "        new_seeds = trackseeding()  \n",
    "             \n",
    "        #Adding new seeds to tracks \n",
    "        tracks    = tracks + new_seeds \n",
    "\n",
    "        # REASIGNING VALUES    \n",
    "        modules[M_i - 1] = left_mod.copy( deep=True)           \n",
    "        modules[M_i    ] = mod.copy(      deep=True)              \n",
    "        modules[M_i + 1] = right_mod.copy(deep=True)        \n",
    "\n",
    "        # Defining a new module.  \n",
    "        work_module      = modules[M_i - 2].copy(deep=True) \n",
    "        #print(work_module)\n",
    "      \n",
    "        #print(left_mod['used'], mod['used'], right_mod['used'])\n",
    "        \n",
    "        new_tracks       = track_forwarding()         \n",
    "        tracks           = new_tracks\n",
    "        #Reasigning \n",
    "        modules[M_i - 2] = work_module.copy(deep=True)\n",
    "        #\n",
    "        t2 = time.time()\n",
    "        print(\"time per module\", t2-t1)     \n",
    "        \n",
    "    #tracks = tracks + frozen_tracks    \n",
    "    print(\"FINDING TRACKS FINISHED\") \n",
    "    T2 = time.time()# Timing The Run-Time\n",
    "    print(\"RUN TOTAL TIME PER EVENT IS : \", T2-T1) \n",
    "        \n",
    "    df_real_tracks = df.groupby(['particle_id'])['hit_id'].unique()       # this is a series.\n",
    "    # This are the reconstructible Particles \n",
    "    #df_real_tracks = df_real_tracks[df_real_tracks.apply(len) > 2]\n",
    "\n",
    "    #if len(tracks) == 0 or len(weak_tracks) == 0  or len(frozen_tracks) == 0:\n",
    "    #print(\"the tracks are zero. Please try another configuration again.\")\n",
    "    #else : \n",
    "    if len(tracks) > 0 :\n",
    "        print(\"SCORING using tracks\")              \n",
    "        print(Scoring(df_real_tracks, tracks))    \n",
    "    elif len(weak_tracks) > 0 :  \n",
    "        print(\"SCORING using weak_tracks\")\n",
    "        print(Scoring(df_real_tracks, weak_tracks))\n",
    "    elif len(frozen_tracks) > 0 : \n",
    "        print(\"SCORING using frozen_tracks\")  \n",
    "        print(Scoring(df_real_tracks, frozen_tracks))\n",
    "    elif len( tracks + frozen_tracks + weak_tracks ) > 0 : \n",
    "        print(\"SCORING using all tracks\")  \n",
    "        print(Scoring(df_real_tracks, tracks + frozen_tracks + weak_tracks))\n",
    "    else :\n",
    "        print(\"Absolutely no track is founded: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_by_triplet(TIME_RESOLUTION = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doing_plots(df, weak_tracks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
