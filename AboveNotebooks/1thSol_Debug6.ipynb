{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from split import *\n",
    "from score import *\n",
    "from scipy import interpolate\n",
    "import time \n",
    "%matplotlib inline \n",
    "import warnings \n",
    "warnings.simplefilter('ignore', np.RankWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rho(x,y):\n",
    "    return np.sqrt(x*x + y*y)\n",
    "def r(x,y,z):\n",
    "    return np.sqrt(x*x + y*y + z*z)\n",
    "def theta(x,y,z):\n",
    "    return np.arccos(z/r(x,y,z))\n",
    "def phi(x,y):\n",
    "    return np.arctan(y/x)\n",
    "def module(r):\n",
    "    return np.sqrt(np.sum(r*r))\n",
    "def r_e(z, r_l, r_c):\n",
    "    z_c = r_c[2] \n",
    "    r_versor = (r_l - r_c)/module(r_l - r_c)               # computing r_versor\n",
    "    r_versor_dot_z_versor = r_versor[2]  \n",
    "    return r_c - r_versor/r_versor_dot_z_versor*(z_c - z)  # IMPORTANT WITH THE MINUS SIGN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_data(fraction):\n",
    "    name = 'data/RAMPData55microns50psInner200microns50psOuter_train.txt' # To be modified for others. \n",
    "    data_fraction = fraction\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df = pd.read_csv(name, sep=' ') # All data.\n",
    "    df,_ = Split_frac(df, data_fraction)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATAFRAME will be a global data. \n",
    "fraction = 0.004 \n",
    "dphi     = 0.01 \n",
    "df_original = reading_data(fraction)  \n",
    "df          = df_original        \n",
    "df_search   = df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortbyphi(df):\n",
    "    '''Description:\n",
    "    Sort each D_i increasingly accoording to phi\n",
    "    '''\n",
    "    z = np.sort(df['z'].unique())\n",
    "    df['phi'] = np.arctan(df['x']/df['y'])\n",
    "    \n",
    "    modules = [] \n",
    "    \n",
    "    for layer_i in z[::-1] :\n",
    "        tmp = df.query(f'z=={layer_i}')\n",
    "        tmp = tmp.sort_values('phi', ascending=True)\n",
    "        \n",
    "        tmp['used'] = False  # To accept or To Neglect\n",
    "\n",
    "        modules.append(tmp)\n",
    "    \n",
    "    return modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findcandidatewindows(left_mod, mod, right_mod, dphi):\n",
    "    '''Description: \n",
    "        Compute the first and last candidates(the window) according to acceptance range(dphi) for each hit.\n",
    "        SUPPOSSING THAT ALL DATA ARE ORDERED ACCOURDING TO PHI. THIS PROCCESS WAS DONE Previously\n",
    "        In case of add more information to the modules, one easily can add throught the iteration \n",
    "    '''\n",
    "    \n",
    "    # CONVENTION:     \n",
    "    # l_m  m  r_m   the values are ordered.      \n",
    "    #  |   |   |             \n",
    "    #  |   |   |    phi up  \n",
    "    #  |   |   |    phi      \n",
    "    #  |   |   |    phi down \n",
    "    #  |   |   |          \n",
    "    \n",
    "    right_hit_max = [] \n",
    "    right_hit_min = [] \n",
    "\n",
    "    temporal = mod['phi'] \n",
    "    \n",
    "    # ITERATION OVER PHI FOR RIGHT \n",
    "    \n",
    "    for phi_i in mod['phi']: \n",
    "        #print(\"=\")\n",
    "        #print(phi_i)\n",
    "        if str(phi_i) == 'nan' :     \n",
    "            print(phi_i, \"the value of phi_i is NaN ON RIGHT\")\n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            right_hit_min.append(m) \n",
    "            right_hit_max.append(M) \n",
    "            continue # \n",
    "        if str(phi_i) == 'NaN' :     \n",
    "            print(phi_i, \"the value of phi_i is NaN ON RIGHT\")\n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            left_hit_min.append(m) \n",
    "            left_hit_max.append(M) \n",
    "            continue # \n",
    "        # GET HIT \n",
    "        down      = phi_i - dphi \n",
    "        up        = phi_i + dphi \n",
    "        #print(down, up)\n",
    "        \n",
    "        condition = f'{down} <= phi <=  {up}'\n",
    "        tmp_df = right_mod.query(condition)\n",
    "        if not tmp_df.empty:\n",
    "            m = tmp_df['hit_id'][tmp_df.index[0]]     # minumum hit \n",
    "            M = tmp_df['hit_id'][tmp_df.index[-1]]    # maximum hit \n",
    "            right_hit_min.append(m) \n",
    "            right_hit_max.append(M) \n",
    "        elif tmp_df.empty :\n",
    "\n",
    "            m = \"nan\" #pd.np.nan                      # minumum hit \n",
    "            M = \"nan\" #pd.np.nan                      # maximum hit\n",
    "            right_hit_min.append(m)  \n",
    "            right_hit_max.append(M) \n",
    "\n",
    "\n",
    "            \n",
    "    left_hit_max = [] \n",
    "    left_hit_min = [] \n",
    "    # ITERATION OVER PHI FOR LEFT\n",
    "    for phi_i in mod['phi']:\n",
    "        if str(phi_i) == 'NaN' :     \n",
    "            print(phi_i, \"the value of phi_i is NaN ON LEFT\")\n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            left_hit_min.append(m) \n",
    "            left_hit_max.append(M) \n",
    "            continue # \n",
    "        if str(phi_i) == 'nan' :     \n",
    "            print(phi_i, \"the value of phi_i is NaN ON left\")\n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            left_hit_min.append(m) \n",
    "            left_hit_max.append(M) \n",
    "            continue # \n",
    "        \n",
    "        # GET HIT \n",
    "        down      = phi_i - dphi \n",
    "        up        = phi_i + dphi \n",
    "        condition = f'{down} <= phi <= {up}'\n",
    "        tmp_df = left_mod.query(condition)\n",
    "        #print(\"len LEFT\", len(tmp_df))\n",
    "        if not tmp_df.empty :\n",
    "            m = tmp_df['hit_id'][tmp_df.index[0]]        # minumum hit \n",
    "            M = tmp_df['hit_id'][tmp_df.index[-1]]       # maximum hit  \n",
    "            left_hit_min.append(m)\n",
    "            left_hit_max.append(M)\n",
    "        elif tmp_df.empty :\n",
    "            print(\"data_frame is empty LEFT\")\n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            left_hit_min.append(m) \n",
    "            left_hit_max.append(M) \n",
    "            \n",
    "    mod[\"right_hit_max\"] = right_hit_max  \n",
    "    mod[\"right_hit_min\"] = right_hit_min  \n",
    "    mod[\"left_hit_max\"]  = left_hit_max   \n",
    "    mod[\"left_hit_min\"]  = left_hit_min   \n",
    "    \n",
    "    \n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trackseeding():\n",
    "    global left_mod, mod, right_mod   \n",
    "    '''Description: \n",
    "        Checks the preceding and following modules for compatible hits using the above results.\n",
    "        \n",
    "        All triplets in the search window are fitted and compared.\n",
    "        \n",
    "        and the best one is kept as a track seed.\n",
    "        \n",
    "        stores its best found triplet\n",
    "        Finding triplets is ap- plied in first instance to the modules\n",
    "        that are further apart from the collision point\n",
    "        Each triplet is the seed of a forming track\n",
    "    '''\n",
    "    #Necessary functions.\n",
    "    def fit(triplet): \n",
    "        phi_data = [ df.query(f'hit_id == {hit}')['phi'] for hit in triplet ]\n",
    "        z_data   = [ df.query(f'hit_id == {hit}')['z'] for hit in triplet   ]\n",
    "        phi_data = [ hit.values[0] for hit in phi_data                      ]                        \n",
    "        z_data   = [ hit.values[0] for hit in z_data                        ]                    \n",
    "        \n",
    "        # Kind of fit: Linear\n",
    "        fitting = np.polyfit(phi_data, z_data, 1)\n",
    "        # IMPORTANT \n",
    "        # IMPORTANT \n",
    "        # REMEMBER TO PUT HERE THE VALUES OF SIGMA. \n",
    "        # IMPORTANT\n",
    "        chiSquared = np.sum((np.polyval(fitting, z_data) - phi_data)**2) \n",
    "        return chiSquared    \n",
    "    df_triplets = [] \n",
    "    for index, part in mod.iterrows():\n",
    "\n",
    "        r_hit_max, r_hit_min = part[\"right_hit_max\"], part[\"right_hit_min\"]  \n",
    "        l_hit_max, l_hit_min = part[\"left_hit_max\"],  part[\"left_hit_min\" ] \n",
    "        \n",
    "        if r_hit_max is \"nan\":\n",
    "            #print(r_hit_max)\n",
    "            #print(\"pass NAN r_hit_max\")\n",
    "            continue \n",
    "        elif r_hit_min is \"nan\":\n",
    "            #print(r_hit_min)\n",
    "            #print(\"pass NAN r_hit_min\")\n",
    "            continue \n",
    "        elif l_hit_max is \"nan\":\n",
    "            #print(l_hit_max)\n",
    "            #print(\"pass NAN l_hit_max\")\n",
    "            continue \n",
    "        elif l_hit_min is \"nan\":\n",
    "            #print(l_hit_min)\n",
    "            #print(\"pass NAN l_hit_min\")\n",
    "            continue     \n",
    "        r_phi_max = right_mod.query(f\"hit_id == {r_hit_max}\")['phi'].values[0]\n",
    "        r_phi_min = right_mod.query(f\"hit_id == {r_hit_min}\")['phi'].values[0] \n",
    "        \n",
    "        l_phi_max = left_mod.query(f\"hit_id == {l_hit_max}\")['phi'].values[0]  \n",
    "        l_phi_min = left_mod.query(f\"hit_id == {l_hit_min}\")['phi'].values[0]  \n",
    "        \n",
    "        \n",
    "        \n",
    "        left_mod.query(f\" {l_phi_min} <= phi <= {l_phi_max}\")\n",
    "        #Forming all Triplets. \n",
    "        # Here I am adding the condition of used and not used.\n",
    "        # After of this I need to change the condition of used and not used. \n",
    "        #  \n",
    "        # \n",
    "        #  \n",
    "        # \n",
    "        #print(\"tmp_left\")\n",
    "        #print(tmp_left)\n",
    "        #tmp_left = left_mod.query(f\" {l_phi_min} <= phi <= {l_phi_max}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        tmp_right = right_mod.query(f\" {r_phi_min} <= phi <= {r_phi_max} & used == False  \")\n",
    "        for R in tmp_right['phi'].values:\n",
    "        #for L in tmp_left['phi'].values: \n",
    "            #tmp_right = right_mod.query(f\" {r_phi_min} <= phi <= {r_phi_max}\")\n",
    "            #tmp_right = right_mod.query(f\" {r_phi_min} <= phi <= {r_phi_max}\")\n",
    "            tmp_left = left_mod.query(f\" {l_phi_min} <= phi <= {l_phi_max} & used == False \")\n",
    "            #print(\"tmp_right\")\n",
    "            #print(tmp_right)\n",
    "            #for R in tmp_right['phi'].values:\n",
    "            #tmp_left = left_mod.query(f\" {l_phi_min} <= phi <= {l_phi_max}\")\n",
    "            for L in tmp_left['phi'].values: \n",
    "                \n",
    "                # I WILL SUPPOSE THAT \n",
    "                # All information it is found on the hit values.\n",
    "                #print(\"VALUES  center left right \")\n",
    "                hit_center = int(part[\"hit_id\"])  \n",
    "                #print(part[\"hit_id\"])\n",
    "                hit_left   = int(tmp_left.query(f\" phi == {L}\")['hit_id'].values[0] )    \n",
    "                #print(tmp_left.query(f\" phi == {L}\")['hit_id'].values )\n",
    "                hit_right  = int(tmp_right.query(f\" phi == {R}\")['hit_id'].values[0] )                          \n",
    "                #print(tmp_right.query(f\" phi == {R}\")['hit_id'].values)\n",
    "                # With this data we have built the triplets. \n",
    "                triplets = [hit_left, hit_center, hit_right]\n",
    "                # This a lost of memory. I mean that call by hits and not by values is a lost \n",
    "                # of memory.\n",
    "                #print(\"triplets\")\n",
    "                #print(\"triplets\")\n",
    "                #print(\"triplets\")\n",
    "                #print(triplets)\n",
    "                chi2 = fit(triplets)                                                                                                                                                                \n",
    "                # Finally we append the values of the data to a df_triplets\n",
    "                \n",
    "                #print(\"####?###\")\n",
    "                #print(triplets)\n",
    "                #print(chi2)\n",
    "                #print(\"#######\")\n",
    "                df_triplets.append(list(triplets)+[chi2])\n",
    "    df_triplets = pd.DataFrame(df_triplets, columns = ['left_hit', 'hit', 'right_hit', 'chi2']) \n",
    "    #        print(\"=========\")\n",
    "    #        print(df_triplets)\n",
    "    #        print(\"=========\")    \n",
    "    # Up to this point it is necessary to have the values of df_triplets complete\n",
    "    # Then the algorithm should continue to get the best choices according to the values\n",
    "    # of chi2.  \n",
    "              \n",
    "    def best_choice(df_triplets):\n",
    "        seeds = []\n",
    "        for hit_c in df_triplets['hit'].values :\n",
    "            #print(hit_c)\n",
    "            tmp = df_triplets.query(f'hit == {hit_c}')\n",
    "            #print(tmp)\n",
    "            minimum = (tmp['chi2']).idxmin()\n",
    "            #i = chi2values.idxmin() # Here we choice the values.\n",
    "            #tracks.append((tmp.loc[i]).values) \n",
    "            t = (tmp.loc[minimum]).values \n",
    "            t = [int(i) for i in t]\n",
    "            \n",
    "            \n",
    "       \n",
    "            ######################MARKING TRIPLES###########\n",
    "            # MARKING EACH HIT AS USED ON THE WORKING MODULE  # LEFT\n",
    "            hit_id_left = t[0] \n",
    "            index_left   = df.query(f\"hit_id == {hit_id_left}\").index[0]   #################### df works but it would be best to use\n",
    "            left_mod.loc[index_left, \"used\"]   = True         #MARKING USED VALUE\n",
    "            ################################################  # CENTER\n",
    "            # MARKING EACH HIT AS USED ON THE WORKING MODULE \n",
    "            hit_id_center = t[1] \n",
    "            index_center = df.query(f\"hit_id == {hit_id_center}\").index[0]\n",
    "            mod.loc[index_center, \"used\"]      = True            #MARKING USED VALUE\n",
    "            ################################################  # RIGHT\n",
    "            # MARKING EACH HIT AS USED ON THE WORKING MODULE\n",
    "            hit_id_right = t[2] \n",
    "            index_right  = df.query(f\"hit_id == {hit_id_right}\").index[0]\n",
    "            right_mod.loc[index_right, \"used\"] = True         #MARKING USED VALUE\n",
    "            ################################################\n",
    "            ###########################################################\n",
    "            #these are the triplets\n",
    "            #print(t[:3])\n",
    "            #seeds.insert(0, list(t[:3]))\n",
    "            seeds.append(list(t[:3]))    # Here I am negleting the information chi2 because is not important\n",
    "            #print(t[:3])\n",
    "        return seeds # obviously it is a track\n",
    "    \n",
    "    seeds = best_choice(df_triplets)\n",
    "    return seeds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_forwarding():\n",
    "    global tracks, work_module, dphi # this step is important, because I'am using the values of work_module at the same time to modify.\n",
    "    new_tracks = [] \n",
    "    # Notation:\n",
    "    # x0, y0, z0 is the EXTRAPOLATED track.               \n",
    "    # X,  Y,  Z  is the last track on previous module.   \n",
    "    # x,  y,  z  is the tracks on a window.                                                                 \n",
    "    # Searching tracks on phi_e - dphi < phi < d that minimize the extrapolated function.\n",
    "    # r0 = np.array([x0, y0, z0] )\n",
    "    # r  = np.array([x, y, 1] )\n",
    "    # R  = np.array([X,  Y,  Z ] )\n",
    "    def mod(r):\n",
    "        return np.sqrt(np.sum(r*r))\n",
    "    def ext_func(r0, r1, r):\n",
    "        # r0, r1, r are arrays\n",
    "        dx2_plus_dy2 = mod(r0-r)  # distance between hits on the working module.  \n",
    "        dz2          = mod(r1-r0) # distance between the last two modules.                                  \n",
    "        return dx2_plus_dy2/dz2   \n",
    "    \n",
    "    z_e = float(work_module['z'].unique()) #z_position of work_module\n",
    "    #phi_e \n",
    "    #x_e \n",
    "    #y_e \n",
    "    #z_e \n",
    "    # here the track is exactly the seed. Only for the 1th iteration\n",
    "    for track in tracks: \n",
    "        #PROOF: Do have the track values information of USED ?\n",
    "        data = []          \n",
    "        vector_data = []                                                                                                                    \n",
    "        for hit in track[0:2] :                                       \n",
    "            data.append(tuple((df.query(f'hit_id == {hit}')[['phi', 'z']]).values[0]))     \n",
    "            vector_data.append(tuple((df.query(f'hit_id == {hit}')[['x', 'y', 'z']]).values[0]))\n",
    "        phi_data, z_data = zip(*data) \n",
    "        #print(vector_data)\n",
    "        #EXTRAPOLATED SEGMENT FUNCTION  \n",
    "        ext_seg = interpolate.interp1d(z_data, phi_data, fill_value = \"extrapolate\" )\n",
    "        phi_e = ext_seg(z_e)\n",
    "        \n",
    "        r_l, r_c = vector_data \n",
    "        r_l, r_c = np.array(r_l), np.array(r_c)\n",
    "        \n",
    "        x_e, y_e, z_e = r_e(z_e, r_l, r_c) # COMPUTING THE VALUES ON THE WORKING MODULE. It is good.\n",
    "        \n",
    "        #PROOF, HERE THE VALUE OF Z_E SHOULD BE THE SAME ON BOTH ********** IMPORTANT \n",
    "        \n",
    "        #depend on z_e\n",
    "        #y_e = #depend on z_e      \n",
    "        \n",
    "        #Open a Window centered on phi_e: \n",
    "        \n",
    "        down = phi_e - dphi\n",
    "        up   = phi_e + dphi   \n",
    "        \n",
    "        #################################### WINDOW ###########################\n",
    "        df_work_module_window = work_module.query(f\" {down} <= phi <= {up} \" ) # I do not know how much time this line take \n",
    "\n",
    "        #This definition will be done after the loop.\n",
    "        #df_candidates = pd.DataFrame(columns=[\"hit_id\", \"ext_fun\"]) # This dataframe have to be reviwed\n",
    "        hit_left = track[0]   \n",
    "        R  = df.query(f'hit_id == {hit_left}')[['x','y','z']].values[0] # this value would have to change on each track\n",
    "        r0 = np.array([x_e, y_e ,z_e ])\n",
    "\n",
    "        #print(R, \"this is R\")\n",
    "        #print(r0, \"this is r0\")\n",
    "        tmp_candidates = []\n",
    "        for index, row in df.iterrows(): # here the index is not important\n",
    "            # Here I only need to have the information of position.\n",
    "            r      =  row[['x', 'y', 'z']].values # this value is variable on each row.\n",
    "            hit_id =  row['hit_id']    \n",
    "            \n",
    "            #print(\"PROOF\")\n",
    "            #print(r, hit_id)\n",
    "            #Then I compute the value \n",
    "            ext_func_value = ext_func(r0, R, r)\n",
    "            tmp_candidates.append( [hit_id, ext_func_value] )\n",
    "            #print(tmp_candidates)\n",
    "        \n",
    "        if tmp_candidates == []:                                            \n",
    "            print(\"an error ocurred: with tmp_candidates is empty \")                                          \n",
    "            break \n",
    "        #print(\"=====\")\n",
    "        #print(tmp_candidates)\n",
    "        #print(\"=====\")\n",
    "        df_candidates = pd.DataFrame(tmp_candidates, columns=[\"hit_id\", \"ext_fun\"])\n",
    "        # I will find the minimum value of the extrapolation function \n",
    "        #print(\"df_candidadtes \")\n",
    "        #print(df_candidates['ext_fun'])\n",
    "        #print(\"df_candidadtes\")    \n",
    "        index      = df_candidates['ext_fun'].idxmin()\n",
    "        new_hit_id = df_candidates['hit_id'][index]\n",
    "        new_hit_id = int(new_hit_id)\n",
    "        \n",
    "        # MARKING EACH HIT AS USED ON THE WORKING MODULE \n",
    "        \n",
    "        work_module.loc[index, \"used\"] = True \n",
    "        new_track  = [new_hit_id] + track\n",
    "        new_tracks.append(new_track)\n",
    "    return new_tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module number 24\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'right_hit_max'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/afs/cern.ch/work/p/ppayemam/miniconda/envs/ramp_velo_challenge/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4380\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4381\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlibindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4382\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_box\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.validate_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-8b40b5508865>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mnew_seeds\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtrackseeding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mtracks\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mtracks\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnew_seeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mwork_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM_i\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-181-774e41de1150>\u001b[0m in \u001b[0;36mtrackseeding\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mr_hit_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_hit_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"right_hit_max\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"right_hit_min\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0ml_hit_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_hit_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"left_hit_max\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"left_hit_min\"\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/work/p/ppayemam/miniconda/envs/ramp_velo_challenge/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/work/p/ppayemam/miniconda/envs/ramp_velo_challenge/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4387\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4388\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4389\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4390\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4391\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/work/p/ppayemam/miniconda/envs/ramp_velo_challenge/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 4375\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   4376\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'right_hit_max'"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "################################# MAIN #################################################\n",
    "########################################################################################\n",
    "############################ GENERAL ALGORITHM #########################################\n",
    "#####PARAMETERS#####\n",
    "fraction = 0.004    \n",
    "dphi     = 0.1  \n",
    "m        = 22  # number of modules counted from the left.\n",
    "               # from 1 to 24. No more. \n",
    "    \n",
    "new_tracks = []                      # \n",
    "df = reading_data(fraction)          # where data is unmodified.\n",
    "# df_search   = df_original          # where I am searching \n",
    "tracks = []                          # [[1,24, 5], [7,6,4] ,[346,7,32,], ... ]\n",
    "# *********************IMPORTANT**********************************************\n",
    "# The information of tracks is ordered. \n",
    "# Because each of its elements are an ordered list according to module layers.\n",
    "# However, the information of hits are unique and not matter if are a ordered set. \n",
    "# But it was filled out in order\n",
    "\n",
    "\n",
    "\n",
    "# SEPARATION BY MODULE\n",
    "modules = sortbyphi(df) \n",
    "for M_i in range(len(modules)-1, len(modules)-m-1, -1):\n",
    "#for M_i in range(22, 20, -1):\n",
    "    t1 = time.time()\n",
    "    print(f\"module number {M_i}\")\n",
    "    M_i = M_i - 1\n",
    "    #1th STEP: module #m-1 = M_i\n",
    "    left_mod  =  modules[M_i - 1]  \n",
    "    mod       =  modules[M_i    ]  \n",
    "    right_mod =  modules[M_i + 1] \n",
    "    \n",
    "\n",
    "    new_seeds   = trackseeding() \n",
    "    tracks      = tracks + new_seeds\n",
    "    work_module = modules[M_i - 2]    \n",
    "    new_tracks  = track_forwarding()\n",
    "\n",
    "\"\"\"\n",
    "    t2 = time.time()\n",
    "    print(\"time per module\", t2-t1) \n",
    "tracks = pd.Series(new_tracks)\n",
    "print(\"FINDING TRACKS FINISHED \")\n",
    "######################\n",
    "######  FINALLY  #####\n",
    "###### COMPARING #####\n",
    "######################\n",
    "df_real_tracks = df.groupby(['particle_id'])['hit_id'].unique() # this is a series.\n",
    "#Scoring(df_real_tracks, tracks) \n",
    "######################\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tracks     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring(df_real_tracks, new_tracks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
