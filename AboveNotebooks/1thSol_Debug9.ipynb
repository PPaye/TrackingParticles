{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTitle:       Search_By_Triplet.\\n\\nAutor:       Piter Amador Paye Mamani.\\n\\nDescription:\\n             This python-code is an implementation of the algorithm Search By Triplet that was inspired on the work of\\n             Daniel Campora.           \\nChanges:\\n1.  Squeletum of the algorithm\\n2.  First Tracks. \\n3. \\n4. \\n5.  Adding exceptions. \\n6.  I've deleted unnecessary comments. Also, I was getting an error at time to compute findcandidatewindows.\\n    Problems. One get the values of tracks \\n7.  Changing the jerarquy of the function, according to the paper. It means that findcandidatewindows is calculated\\n    on all modules befero they were processed.\\n6. \\n7. \\n8.  I've added the information of weak_tracks and I've added the information of USED and NOt USED \\n9.  dphi  is a constant value\\n9.  THIS IS THE MOST STABLE VERSION. IT TAKES THE ALGORITHM OF DANIEL CAMPORA. AND THEN MAKE A DIFFERENT PREDICITON. IN A SENSE LIKE TAKE A CONSTATNT VALUE OF THE DPHI. \\n    IMPORTANT: STABLE VERSION.\\n    IMPORTANT: STABLE VERSION. \\n    IMPORTANT: STABLE VERSION. \\n    IMPORTANT: STABLE VERSION. \\n    IMPORTANT: STABLE VERSION.\\n\\n\\n11. \\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from split import *\n",
    "from score import *\n",
    "from scipy import interpolate\n",
    "import time \n",
    "%matplotlib inline \n",
    "import warnings \n",
    "warnings.simplefilter('ignore', np.RankWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Title:       Search_By_Triplet.\n",
    "\n",
    "Autor:       Piter Amador Paye Mamani.\n",
    "\n",
    "Description:\n",
    "             This python-code is an implementation of the algorithm Search By Triplet that was inspired on the work of\n",
    "             Daniel Campora.           \n",
    "Changes:\n",
    "1.  Squeletum of the algorithm\n",
    "2.  First Tracks. \n",
    "3. \n",
    "4. \n",
    "5.  Adding exceptions. \n",
    "6.  I've deleted unnecessary comments. Also, I was getting an error at time to compute findcandidatewindows.\n",
    "    Problems. One get the values of tracks \n",
    "7.  Changing the jerarquy of the function, according to the paper. It means that findcandidatewindows is calculated\n",
    "    on all modules befero they were processed.\n",
    "6. \n",
    "7. \n",
    "8.  I've added the information of weak_tracks and I've added the information of USED and NOt USED \n",
    "9.  dphi  is a constant value\n",
    "9.  THIS IS THE MOST STABLE VERSION. IT TAKES THE ALGORITHM OF DANIEL CAMPORA. AND THEN MAKE A DIFFERENT PREDICITON. IN A SENSE LIKE TAKE A CONSTATNT VALUE OF THE DPHI. \n",
    "    IMPORTANT: STABLE VERSION.\n",
    "    IMPORTANT: STABLE VERSION. \n",
    "    IMPORTANT: STABLE VERSION. \n",
    "    IMPORTANT: STABLE VERSION. \n",
    "    IMPORTANT: STABLE VERSION.\n",
    "\n",
    "\n",
    "11. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rho(x,y):\n",
    "    return np.sqrt(x*x + y*y)\n",
    "def r(x,y,z):\n",
    "    return np.sqrt(x*x + y*y + z*z)\n",
    "def theta(x,y,z):\n",
    "    return np.arccos(z/r(x,y,z))\n",
    "def phi(x,y):\n",
    "    return np.arctan(y/x)\n",
    "def module(r):\n",
    "    return np.sqrt(np.sum(r*r))\n",
    "def r_e(z, r_l, r_c):\n",
    "    z_c = r_c[2] \n",
    "    r_versor = (r_l - r_c)/module(r_l - r_c)               # computing r_versor\n",
    "    r_versor_dot_z_versor = r_versor[2]  \n",
    "    return r_c - r_versor/r_versor_dot_z_versor*(z_c - z)  # IMPORTANT WITH THE MINUS SIGN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_data(fraction):\n",
    "    name = 'data/RAMPData55microns50psInner200microns50psOuter_train.txt' # To be modified for others. \n",
    "    data_fraction = fraction\n",
    "    df = pd.DataFrame()\n",
    "    df = pd.read_csv(name, sep=' ')      # All data.\n",
    "    df,_ = Split_frac(df, data_fraction)            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data/RAMPData55microns50psInner200microns50psOuter_train.txt' does not exist: b'data/RAMPData55microns50psInner200microns50psOuter_train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6b054618807c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DATAFRAME will be a global data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.04\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mreading_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-66ce110f5928>\u001b[0m in \u001b[0;36mreading_data\u001b[0;34m(fraction)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata_fraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# All data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSplit_frac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_fraction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/work/p/ppayemam/miniconda/envs/ramp_velo_challenge/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/work/p/ppayemam/miniconda/envs/ramp_velo_challenge/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/work/p/ppayemam/miniconda/envs/ramp_velo_challenge/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/work/p/ppayemam/miniconda/envs/ramp_velo_challenge/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/work/p/ppayemam/miniconda/envs/ramp_velo_challenge/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/RAMPData55microns50psInner200microns50psOuter_train.txt' does not exist: b'data/RAMPData55microns50psInner200microns50psOuter_train.txt'"
     ]
    }
   ],
   "source": [
    "# DATAFRAME will be a global data. \n",
    "fraction = 0.04\n",
    "df       = reading_data(fraction)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortbyphi(df):\n",
    "    '''Description:\n",
    "    Sort each D_i increasingly accoording to phi\n",
    "    And add a column to the dataframe_module with the name of used to accept or neglect hits. \n",
    "    '''\n",
    "    z = np.sort(df['z'].unique())\n",
    "    df['phi'] = np.arctan(df['x']/df['y'])\n",
    "    modules = [] \n",
    "    for layer_i in z[::1] :\n",
    "        tmp = df.query(f'z=={layer_i}')\n",
    "        tmp = tmp.sort_values('phi', ascending=True)\n",
    "        # IMPORTANT \n",
    "        tmp['used'] = False # MATCHING ALL HITS IN THE MODULES LIKE NOT USED.\n",
    "        modules.append(tmp)  \n",
    "    return modules       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI = []\n",
    "def findcandidatewindows(left_mod, mod, right_mod ):\n",
    "    # phi_window     =  phi_extrapolation_base + np.abs( hit_Zs[h_center]) * phi_extrapolation_coef \n",
    "    global phi_extrapolation_coef, phi_extrapolation_base , dphi\n",
    "    '''Description: \n",
    "        Compute the first and last candidates(the window) according to acceptance range(dphi) for each hit.\n",
    "        SUPPOSSING THAT ALL DATA ARE ORDERED ACCOURDING TO PHI. THIS PROCCESS WAS DONE Previously\n",
    "        In case of add more information to the modules, one easily can add throught the iteration \n",
    "    '''\n",
    "    # CONVENTION :     \n",
    "    # l_m  m  r_m   the values are ordered.      \n",
    "    #  |   |   |             \n",
    "    #  |   |   |    phi up  \n",
    "    #  |   |   |    phi      \n",
    "    #  |   |   |    phi down \n",
    "    #  |   |   |          \n",
    "    \n",
    "    right_hit_max = [] \n",
    "    right_hit_min = [] \n",
    "\n",
    "    temporal = mod['phi'] \n",
    "    \n",
    "    # ITERATION OVER PHI FOR RIGHT \n",
    "    \n",
    "    for phi_i in mod['phi']: \n",
    "        #print(\"=\")\n",
    "        #print(phi_i)\n",
    "        if str(phi_i) == 'nan' :     \n",
    "            #print(phi_i, \"the value of phi_i is NaN ON RIGHT\")\n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            right_hit_min.append(m) \n",
    "            right_hit_max.append(M) \n",
    "            continue # \n",
    "        if str(phi_i) == 'NaN' :     \n",
    "            #print(phi_i, \"the value of phi_i is NaN ON RIGHT\")\n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            left_hit_min.append(m) \n",
    "            left_hit_max.append(M) \n",
    "            continue # \n",
    "            \n",
    "        \n",
    "        z_center = mod['z'].unique()[0]\n",
    "        #z = df.query(f\"phi=={phi_i}\")[\"z\"].values[0]\n",
    "        # GET HIT \n",
    "        \"\"\" dphi =  phi_extrapolation_base + np.abs( z_center ) * phi_extrapolation_coef \"\"\"\n",
    "        \n",
    "        \n",
    "        PHI.append(dphi)\n",
    "        \n",
    "        \n",
    "        \n",
    "        down      = phi_i - dphi \n",
    "        up        = phi_i + dphi \n",
    "        #print(down, up)\n",
    "        \n",
    "        condition = f'{down} <= phi <=  {up}'\n",
    "        tmp_df = right_mod.query(condition)\n",
    "        if not tmp_df.empty:\n",
    "            m = tmp_df['hit_id'][tmp_df.index[0]]     # minumum hit \n",
    "            M = tmp_df['hit_id'][tmp_df.index[-1]]    # maximum hit \n",
    "            right_hit_min.append(m) \n",
    "            right_hit_max.append(M) \n",
    "        elif tmp_df.empty :\n",
    "\n",
    "            m = \"nan\" #pd.np.nan                      # minumum hit \n",
    "            M = \"nan\" #pd.np.nan                      # maximum hit\n",
    "            right_hit_min.append(m)  \n",
    "            right_hit_max.append(M) \n",
    "            \n",
    "    left_hit_max = [] \n",
    "    left_hit_min = [] \n",
    "    # ITERATION OVER PHI FOR LEFT\n",
    "    for phi_i in mod['phi']:\n",
    "        if str(phi_i) == 'NaN' :     \n",
    "            # print(phi_i, \"the value of phi_i is NaN ON LEFT\")\n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            left_hit_min.append(m) \n",
    "            left_hit_max.append(M) \n",
    "            continue # \n",
    "        if str(phi_i) == 'nan' :     \n",
    "            # print(phi_i, \"the value of phi_i is NaN ON left\")\n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            left_hit_min.append(m) \n",
    "            left_hit_max.append(M) \n",
    "            continue # \n",
    "        \n",
    "        # GET HIT \n",
    "        down      = phi_i - dphi \n",
    "        up        = phi_i + dphi \n",
    "        condition = f'{down} <= phi <= {up}'\n",
    "        tmp_df = left_mod.query(condition)\n",
    "        #print(\"len LEFT\", len(tmp_df))\n",
    "        if not tmp_df.empty :\n",
    "            m = tmp_df['hit_id'][tmp_df.index[0]]        # minumum hit \n",
    "            M = tmp_df['hit_id'][tmp_df.index[-1]]       # maximum hit  \n",
    "            left_hit_min.append(m)\n",
    "            left_hit_max.append(M)\n",
    "        elif tmp_df.empty :\n",
    "            # print(\"data_frame is empty LEFT\")\n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            left_hit_min.append(m) \n",
    "            left_hit_max.append(M) \n",
    "            \n",
    "    mod[\"right_hit_max\"] = right_hit_max  \n",
    "    mod[\"right_hit_min\"] = right_hit_min  \n",
    "    mod[\"left_hit_max\"]  = left_hit_max   \n",
    "    mod[\"left_hit_min\"]  = left_hit_min   \n",
    "    return mod\n",
    "\n",
    "###############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trackseeding():\n",
    "    global left_mod, mod, right_mod, M_i, dphi\n",
    "    \n",
    "    '''\n",
    "    Description: \n",
    "        Checks the preceding and following modules for compatible hits using the above results.\n",
    "        \n",
    "        All triplets in the search window are fitted and compared.\n",
    "        \n",
    "        and the best one is kept as a track seed.\n",
    "        \n",
    "        stores its best found triplet\n",
    "        Finding triplets is ap- plied in first instance to the modules\n",
    "        that are further apart from the collision point\n",
    "        Each triplet is the seed of a forming track\n",
    "    '''\n",
    "\n",
    "    #Necessary functions.\n",
    "    def fit(triplet): \n",
    "        phi_data = [ df.query(f'hit_id == {hit}')['phi'] for hit in triplet ]\n",
    "        z_data   = [ df.query(f'hit_id == {hit}')['z']   for hit in triplet ]\n",
    "        phi_data = [ hit.values[0] for hit in phi_data                      ]                        \n",
    "        z_data   = [ hit.values[0] for hit in z_data                        ]                    \n",
    "        # Kind of fit: Linear\n",
    "        fitting = np.polyfit(phi_data, z_data, 1)\n",
    "        # IMPORTANT  \n",
    "        # REMEMBER TO PUT HERE THE VALUES OF SIGMA. \n",
    "        # IMPORTANT \n",
    "        chiSquared = np.sum((np.polyval(fitting, z_data) - phi_data)**2)\n",
    "        return chiSquared\n",
    "\n",
    "    df_triplets = []\n",
    "    \n",
    "    # print(\"len of mod. \",\"in module #\", M_i )\n",
    "    # print(len(mod) )\n",
    "    \n",
    "    for index, part in mod.iterrows():\n",
    "\n",
    "        r_hit_max, r_hit_min = part[\"right_hit_max\"], part[\"right_hit_min\"]  \n",
    "        l_hit_max, l_hit_min = part[\"left_hit_max\"],  part[\"left_hit_min\" ] \n",
    "        \n",
    "        if  str(r_hit_max)  == \"nan\":\n",
    "            #print(r_hit_max)\n",
    "            # print(\"CONTINUE\")\n",
    "            continue \n",
    "        elif str(r_hit_min) == \"nan\":\n",
    "            #print(r_hit_min)\n",
    "            #print(\"pass NAN r_hit_min\")\n",
    "            continue \n",
    "        elif str(l_hit_max) == \"nan\":\n",
    "            #print(l_hit_max)\n",
    "            #print(\"pass NAN l_hit_max\")\n",
    "            continue \n",
    "        elif str(l_hit_min) == \"nan\":\n",
    "            #print(l_hit_min)\n",
    "            #print(\"pass NAN l_hit_min\")\n",
    "            continue  \n",
    "            \n",
    "        if  str(r_hit_max)  == \"NaN\" :\n",
    "            #print(r_hit_max)\n",
    "            #print(\"pass NAN r_hit_max\")\n",
    "            continue \n",
    "        elif str(r_hit_min) == \"NaN\" :\n",
    "            #print(r_hit_min)\n",
    "            #print(\"pass NAN r_hit_min\")\n",
    "            continue \n",
    "        elif str(l_hit_max) == \"NaN\" :\n",
    "            #print(l_hit_max)\n",
    "            #print(\"pass NAN l_hit_max\")\n",
    "            continue \n",
    "        elif str(l_hit_min) == \"NaN\" :\n",
    "            #print(l_hit_min)\n",
    "            #print(\"pass NAN l_hit_min\")\n",
    "            continue  \n",
    "        \n",
    "        r_phi_max = right_mod.query(f\"hit_id == {r_hit_max}\")['phi'].values[0]\n",
    "        r_phi_min = right_mod.query(f\"hit_id == {r_hit_min}\")['phi'].values[0] \n",
    "        \n",
    "        l_phi_max = left_mod.query(f\"hit_id == {l_hit_max}\")['phi'].values[0]  \n",
    "        l_phi_min = left_mod.query(f\"hit_id == {l_hit_min}\")['phi'].values[0]  \n",
    "        \n",
    "        left_mod.query(f\" {l_phi_min} <= phi <= {l_phi_max}\")\n",
    "        #Forming all Triplets. \n",
    "        # Here I am adding the condition of used and not used.\n",
    "        # After of this I need to change the condition of used and not used. \n",
    "        # \n",
    "        #print(\"_left\")\n",
    "        #print(_left)\n",
    "        #tmp_left = left_mod.query(f\" {l_phi_min} <= phi <= {l_phi_max}\")\n",
    "        tmp_right = right_mod.query(f\"   {r_phi_min} <= phi <= {r_phi_max} & used == False  \")    # ADDING TIME\n",
    "        for R in tmp_right['phi'].values:\n",
    "        #for L in tmp_left['phi'].values: \n",
    "            #tmp_right = right_mod.query(f\" {r_phi_min} <= phi <= {r_phi_max}\")\n",
    "            #tmp_right = right_mod.query(f\" {r_phi_min} <= phi <= {r_phi_max}\")\n",
    "            tmp_left = left_mod.query(f\" {l_phi_min} <= phi <= {l_phi_max} & used == False \")     # ADDING TIME\n",
    "            #print(\"tmp_right\")\n",
    "            #print(tmp_right)\n",
    "            #for R in tmp_right['phi'].values:\n",
    "            #tmp_left = left_mod.query(f\" {l_phi_min} <= phi <= {l_phi_max}\")\n",
    "            for L in tmp_left['phi'].values: \n",
    "                \n",
    "                #print(L, hit_center, R)\n",
    "                \n",
    "                # I WILL SUPPOSE THAT \n",
    "                # All information it is found on the hit values.\n",
    "                # print(\"VALUES  center left right \")\n",
    "                \n",
    "                \n",
    "                hit_left   = int( tmp_left.query( f\" phi == {L}\")['hit_id'].values[0]  )  \n",
    "                hit_center = int( part[\"hit_id\"] )\n",
    "                hit_right  = int( tmp_right.query(f\" phi == {R}\")['hit_id'].values[0]  )\n",
    "                                \n",
    "                # With this data we have built the triplets. \n",
    "                triplets = [hit_left, hit_center, hit_right]\n",
    "                \n",
    "                # This a lost of memory. I mean that call by hits and not by values is a lost of memory.\n",
    "                chi2 = fit(triplets)                                                                                                                                                                \n",
    "                # Finally we append the values of the data to a df_triplets\n",
    "                df_triplets.append(list(triplets)+[chi2])\n",
    "                \n",
    "    \n",
    "                \n",
    "    df_triplets = pd.DataFrame(df_triplets, columns = ['left_hit', 'hit', 'right_hit', 'chi2'])  \n",
    "    # Up to this point it is necessary to have the values of df_triplets complete\n",
    "    # Then the algorithm should continue to get the best choices according to the values\n",
    "    # of chi2.\n",
    "    def best_choice(df_triplets):\n",
    "        seeds = []\n",
    "        for hit_c in df_triplets['hit'].unique() : # UNIQUE\n",
    "            # GROUPING \n",
    "            tmp = df_triplets.query(f'hit == {hit_c}')\n",
    "            minimum = (tmp['chi2']).idxmin()\n",
    "            t = (tmp.loc[minimum]).values            \n",
    "            t = [int(i) for i in t]\n",
    "            #these are the triplets       \n",
    "            seeds.append(list(t[:3]))     # Here I am negleting the information chi2 because is not important\n",
    "        return seeds                      # obviously it is a track\n",
    "\n",
    "    seeds = best_choice(df_triplets)\n",
    "    \n",
    "    for seed in seeds:\n",
    "            # #########     MARKING TRIPLES######  \n",
    "            # MATCHING EACH HIT AS USED ON THE WORKING MODULE  \n",
    "            hit_id_left, hit_id_center, hit_id_right = seed\n",
    "            \n",
    "            print(\"the hits that I am getting are correct ???????????????????????????\")\n",
    "            print(seed)\n",
    "            print( (hit_left   in  left_mod['hit_id'].values ) )\n",
    "            print( (hit_center in       mod['hit_id'].values ) )\n",
    "            print( (hit_right  in right_mod['hit_id'].values ) )\n",
    "            print(\"=======================================\")\n",
    "            \n",
    "            #LEFT\n",
    "            left_mod.loc[   left_mod.hit_id == hit_id_left,    \"used\" ]     = True\n",
    "            #CENTER\n",
    "            mod.loc[           mod.hit_id   == hit_id_center,  \"used\" ]     = True\n",
    "            #RIGTH\n",
    "            right_mod.loc[ right_mod.hit_id == hit_id_right,   \"used\" ]     = True\n",
    "    return seeds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACKS = []\n",
    "def track_forwarding():\n",
    "    # Here I have to review the leff_mod\n",
    "    # So, I am beging to print the value of left_mod on three steps \n",
    "    global tracks, work_module, left_mod, mod, right_mod, M_i, weak_tracks \n",
    "    # for \n",
    "    global phi_extrapolation_coef, phi_extrapolation_base \n",
    "\n",
    "    global TRACKS\n",
    "    \n",
    "    new_tracks = [] \n",
    "    # Notation:\n",
    "    # x0, y0, z0 is the EXTRAPOLATED track.               \n",
    "    # X,  Y,  Z  is the last track on previous module.   \n",
    "    # x,  y,  z  is the tracks on a window.                                                                 \n",
    "    # Searching tracks on phi_e - dphi < phi < phi_e + dphi that minimize the extrapolated function.\n",
    "    # r0 = np.array([x0, y0, z0] )\n",
    "    # r  = np.array([x, y, 1] )\n",
    "    # R  = np.array([X,  Y,  Z ] )\n",
    "    def module(r):\n",
    "        return np.sqrt(np.sum(r*r))\n",
    "    def ext_func(r0, r1, r):\n",
    "        # r0, r1, r are arrays\n",
    "        dx2_plus_dy2 = module(  r0-r )     # distance between hits on the working module.  \n",
    "        dz2          = module( r1-r0 )     # distance between the last two modules.                                  \n",
    "        return dx2_plus_dy2/dz2 \n",
    "    \n",
    "    z_e = float(work_module['z'].unique()) #z_position of work_module                    \n",
    "    \n",
    "    # here the track is exactly the seed. Only for the 1th iteration\n",
    "    for track in tracks: \n",
    "        #PROOF: Do you have the track values information of USED ?\n",
    "        data = []          \n",
    "        vector_data = []  \n",
    "        \n",
    "        #EXTRAPOLATION ONLY WITH TWO LAST HITS\n",
    "        \n",
    "        for hit in track[0:2] :\n",
    "            data.append(tuple((df.query(f'hit_id == {hit}')[['phi', 'z']]).values[0]))     \n",
    "            vector_data.append(tuple((df.query(f'hit_id == {hit}')[['x', 'y', 'z']]).values[0]))\n",
    "        phi_data, z_data = zip(*data) \n",
    "        #EXTRAPOLATED SEGMENT FUNCTION      \n",
    "        ext_seg = interpolate.interp1d(z_data, phi_data, fill_value = \"extrapolate\" )\n",
    "        phi_e   = ext_seg(z_e) \n",
    "        r_l, r_c = vector_data                   # THE VALUES ON LEFT AND RIGHT                                                \n",
    "        r_l, r_c = np.array(r_l), np.array(r_c)  # \n",
    "        x_e, y_e, z_e = r_e(z_e, r_l, r_c)       # COMPUTING THE VALUES ON THE WORKING MODULE.                                 \n",
    "        \n",
    "        #Open a Window centered on phi_e: \n",
    "        z_center = mod['z'].unique()[0]\n",
    "        #z = df.query(f\"phi=={phi_i}\")[\"z\"].values[0]\n",
    "        # GET HIT \n",
    "        \"\"\"dphi =  phi_extrapolation_base + np.abs( z_center ) * phi_extrapolation_coef \"\"\"\n",
    "        \n",
    "        down = phi_e - dphi\n",
    "        up   = phi_e + dphi   \n",
    "\n",
    "        if str(down) == 'nan' or str(down) == 'NaN' or str(up) == 'nan' or str(up) == 'NaN' :\n",
    "            print(\"An error ocurred with the values of down or up. Plese cheack.\")\n",
    "            break\n",
    "        \n",
    "        #################################### WINDOW ########################### \n",
    "        df_work_module_window = work_module.query(f\" {down} <= phi <= {up} \" )  \n",
    "        #print(\"df_work_module_window\")\n",
    "        #print(df_work_module_window.head())\n",
    "        \n",
    "        \n",
    "        #This definition will be done after the loop. \n",
    "        #df_candidates = pd.DataFrame(columns=[\"hit_id\", \"ext_fun\"]) # This dataframe have to be reviwed \n",
    "        hit_left = track[0]   \n",
    "        R  = df.query(f'hit_id == {hit_left}')[['x','y','z']].values[0] # this value would have to change on each track\n",
    "        r0 = np.array([x_e, y_e ,z_e ])\n",
    "\n",
    "        tmp_candidates = []\n",
    "        for index, row in df_work_module_window.iterrows(): \n",
    "            # Here I only need to have the information of position.\n",
    "            r      =  row[['x', 'y', 'z']].values # this value is variable on each row.\n",
    "            hit_id =  row['hit_id']    \n",
    "            ext_func_value = ext_func(r0, R, r)\n",
    "            tmp_candidates.append( [hit_id, ext_func_value] )\n",
    "        \n",
    "        #\"************************************************************************************************************\" \n",
    "        #\"************************************If any extrapolated data is not founded on the working module **********\" \n",
    "        #\"************************************************************************************************************\"\n",
    "        #\"**********************************HERE WE ARE LOSING TRACKS***********************************************\"\n",
    "        #\"************************************************************************************************************\"\n",
    "        #\"************************************************************************************************************\"\n",
    "        #\"************************************************************************************************************\"\n",
    "        #\"************************************************************************************************************\"\n",
    "        #\"************************************************************************************************************\"\n",
    "        #\"************************************************************************************************************\"\n",
    "\n",
    "        # this part of the code makes it inefficient\n",
    "        if tmp_candidates == [] :                                             \n",
    "            # In case of don't find a hit on the working module.  \n",
    "            # there are two cases\n",
    "            # = = \n",
    "            \n",
    "            # = = \n",
    "            hit_id_left, hit_id_center, hit_id_right = track[0:3]\n",
    "            \n",
    "            print(len(track))\n",
    "            print(track)\n",
    "            print( (hit_id_left   in       mod['hit_id'].values ) )\n",
    "            print( (hit_id_center in       mod['hit_id'].values ) )\n",
    "            print( (hit_id_right  in right_mod['hit_id'].values ) )\n",
    "            \n",
    "            \n",
    "            \n",
    "            TRACKS.append(track)\n",
    "            # the track has its first forwarding \n",
    "            if ( (hit_id_left in left_mod['hit_id'].values ) ):# and  (hit_id_center in mod['hit_id'].values) and (hit_id_right in right_mod['hit_id'].values ) ) : \n",
    "                same_track     = [] + track  \n",
    "                new_tracks.append(same_track)\n",
    "                continue\n",
    "            # the track has its second worwarding    \n",
    "            elif ( (hit_id_left in mod['hit_id'].values )  ): #and  (hit_center in right_mod['hit_id'].values) ):\n",
    "                weak_tracks.append(track)\n",
    "                # Add to weak_tracks                \n",
    "                continue\n",
    "            else :\n",
    "                continue\n",
    "        # ¿ ******************************************************************************************************** ? \" \n",
    "        # \"************************************************************************************************************\" \n",
    "        # \"************************************tmp_candidates**********************************************************\" \n",
    "        # \"************************************************************************************************************\"\n",
    "                \n",
    "        df_candidates = pd.DataFrame(tmp_candidates, columns=[\"hit_id\", \"ext_fun\"])\n",
    "        \n",
    "        if len(tmp_candidates) == 0 :\n",
    "            print(\"an error ocurred with df_candidates\")\n",
    "            break\n",
    "        \n",
    "        # Choosing new hit_id to complete the track.  \n",
    "        new_hit_id    = df_candidates.loc[df_candidates['ext_fun'].idxmin()]['hit_id']\n",
    "        new_hit_id    = int(new_hit_id)   # new_hit_id   \n",
    "    \n",
    "        # MARKING EACH HIT AS USED ON THE WORKING MODULE        \n",
    "        work_module.loc[ work_module.hit_id == new_hit_id, \"used\" ]  = True   \n",
    "        \n",
    "\n",
    "        new_track     = [new_hit_id] + track  \n",
    "        new_tracks.append(new_track)\n",
    "         \n",
    "    return new_tracks  # this value will be replaced by tracks on the main algorithm\n",
    "\n",
    "\"\"\"\n",
    "#delete\n",
    "append to weaktracks              \n",
    "weak_tracks.append(track)        \n",
    "# MATCHING EACH HIT AS USED ON THE WORKING MODULE  \n",
    "hit_id_left, hit_id_center, hit_id_right = track[0:3] \n",
    "#LEFT\n",
    "left_mod.loc[left_mod.hit_id == hit_id_left, \"used\"]     = True\n",
    "#CENTER\n",
    "mod.loc[        mod.hit_id == hit_id_center, \"used\"]     = True\n",
    "#RIGTH\n",
    "right_mod.loc[right_mod.hit_id == hit_id_right, \"used\"]  = True\n",
    "# I CAN MARK LIKE USED\n",
    "        if len(track) == 3 :\n",
    "            # #########     MARKING TRIPLES######  \n",
    "            # MATCHING EACH HIT AS USED ON THE WORKING MODULE  \n",
    "\n",
    "            hit_id_left, hit_id_center, hit_id_right = track\n",
    "\n",
    "            #LEFT\n",
    "            left_mod.loc[left_mod.hit_id == hit_id_left, \"used\"]     = True\n",
    "            #CENTER\n",
    "            mod.loc[        mod.hit_id == hit_id_center, \"used\"]     = True\n",
    "            #RIGTH\n",
    "            right_mod.loc[right_mod.hit_id == hit_id_right, \"used\"]  = True\n",
    "\n",
    "            # FINALLY we add the new track to the value of tracks\n",
    "            # ADDING to track\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "################################## MAIN ################################################\n",
    "########################################################################################\n",
    "############################ GENERAL ALGORITHM #########################################\n",
    "############################     PARAMETERS    #########################################\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "\n",
    "fraction = 0.04 # \n",
    "dphi     = 0.001  # The windows is a variable quantity that depend on phi_ext_base ###### \n",
    "# phi_window     =  phi_extrapolation_base + np.abs( hit_Zs[h_center]) * phi_extrapolation_coef         ;\n",
    "\n",
    "phi_extrapolation_coef = 0.02\n",
    "phi_extrapolation_base = 0.03   \n",
    "                                     #m = 24    # number of modules counted from the left.  #\n",
    "m = 23                               # from 1 to 24. No more. \n",
    "\n",
    "new_tracks = []                      # \n",
    "df = reading_data(fraction)          # where data is unmodified.\n",
    "# df_search   = df_original          # where I am searching \n",
    "tracks = []                          # [[1,24, 5], [7,6,4] ,[346,7,32,], ... ]\n",
    "weak_tracks  =[]                     # 123\n",
    "# *********************IMPORTANT**********************************************\n",
    "# The information of tracks is ordered. \n",
    "# Because each of its elements are an ordered list according to module layers.\n",
    "# However, the information of hits are unique and not matter if are a ordered set. \n",
    "# But it was filled out in order\n",
    "\n",
    "# SEPARATION BY MODULE  \n",
    "modules = sortbyphi(df)  \n",
    "# FIND CANDIDATE WINDOWS. In order to minimize the amount of candidates considered in subqsequent steps.\n",
    "\n",
    "for M_i in range(len(modules)-1, len(modules)-m-1, -1): \n",
    "    M_i = M_i - 1\n",
    "    left_mod     =  modules[M_i - 1]   \n",
    "    mod          =  modules[M_i    ]  \n",
    "    right_mod    =  modules[M_i + 1]  \n",
    "    modules[M_i] =  findcandidatewindows(left_mod, mod, right_mod)\n",
    "\n",
    "\n",
    "    \n",
    "#ITERATION OVER MODULES ( ):\n",
    "\n",
    "for M_i in range(len(modules)-1-1, len(modules)-m-2, -1):  # the number two is due to 1. index postion default. 2. \n",
    "    print(M_i)\n",
    "#for M_i in range(22, 20, -1):\n",
    "\n",
    "    # TIMING THE RUNNING OVER A MODULE\n",
    "    \n",
    "    t1 = time.time()                \n",
    "    print(f\"module number {M_i}\")\n",
    "    #M_i = M_i - 1\n",
    "    \n",
    "    #1th STEP:  ASIGNING NOTATION\n",
    "    left_mod  =  modules[M_i - 1]  \n",
    "    mod       =  modules[M_i    ]  \n",
    "    right_mod =  modules[M_i + 1] \n",
    "\n",
    "    new_seeds  = trackseeding()     \n",
    "    #Adding new seeds to tracks \n",
    "    tracks = tracks + new_seeds\n",
    "    \n",
    "    # REASIGNING VALUES    \n",
    "    modules[M_i - 1] = left_mod           \n",
    "    modules[M_i    ] = mod              \n",
    "    modules[M_i + 1] = right_mod        \n",
    "\n",
    "    # Defining a new module.  \n",
    "    work_module = modules[M_i - 2] \n",
    "    \n",
    "    #print(left_mod['used'], mod['used'], right_mod['used'])\n",
    "\n",
    "    new_tracks  = track_forwarding() \n",
    "    tracks = new_tracks\n",
    "    \n",
    "    #Reasigning \n",
    "    modules[M_i - 2] = work_module\n",
    "    #\n",
    "    t2 = time.time()\n",
    "    print(\"time per module\", t2-t1) \n",
    "tracks = pd.Series(new_tracks)\n",
    "\n",
    "#print(\"FINDING TRACKS FINISHED\")\n",
    "\n",
    "######################\n",
    "######  FINALLY  #####\n",
    "###### COMPARING #####\n",
    "######################\n",
    "######################\n",
    "\n",
    "output = []\n",
    "for x in tracks:\n",
    "    if x not in output:\n",
    "        output.append(x)\n",
    "    else:\n",
    "        print(\"repeated\")\n",
    "tracks_1 = output\n",
    "\n",
    "df_real_tracks = df.groupby(['particle_id'])['hit_id'].unique() # this is a series.\n",
    "Scoring(df_real_tracks, tracks)\n",
    "#######################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTTING THE VALUES OF THE TRACKS. X-Y, Z-Y, AND SO ON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rc('text', usetex=True)\n",
    "matplotlib.rcParams['text.latex.preamble'] = [r'\\boldmath']\n",
    "\n",
    "# List will be used to create a text file \n",
    "# Create plots to show the reconstructed tracks\n",
    "\n",
    "plt.figure()\n",
    "tracks = tracks.to_list() + weak_tracks \n",
    "for track in tracks:\n",
    "    # Here I can get the values of the orignal dataframe.\n",
    "    data = []\n",
    "    for hit in track : # \n",
    "        data.append(list(df.query(f\"hit_id == {hit}\").values[0])) # what kind of data we want.\n",
    "    \n",
    "    data = pd.DataFrame(data, columns=list(df.columns.values) ) \n",
    "    print(len(data))\n",
    "    plt.plot(data['z'], data['y'], '-', alpha=0.8, lw=2)\n",
    "    plt.scatter(data['z'], data['y'], marker='+' )\n",
    "    plt.scatter(df_real_tracks['z'], df_real_tracks['y'], marker='+' )\n",
    "    plt.xlabel(r\"\\textbf{Z}\")\n",
    "    plt.ylabel(r'\\textbf{Y}')\n",
    "    plt.grid(True)\n",
    "    # tracks.append(data['hit_id'])\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"    \n",
    "# In the z-x plane\n",
    "plt.figure()\n",
    "for vals in np.unique(df['phi'].values):\n",
    "    data = df.query(f'(phi == {vals})')\n",
    "    plt.plot(data['z'], data['x'], '-', alpha=0.5, lw=2)\n",
    "    plt.scatter(data['z'], data['x'], marker='+' )\n",
    "    plt.xlabel(r\"\\textbf{Z}\")\n",
    "    plt.ylabel(r'\\textbf{X}')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure()\n",
    "for vals in np.unique(df['phi'].values):\n",
    "    data = df.query(f'(phi == {vals})')\n",
    "    plt.plot(data['x'], data['y'], '-', alpha=0.5, lw=2)\n",
    "    plt.scatter(data['x'], data['y'], marker='+' )\n",
    "    plt.xlabel(r\"\\textbf{X}\")\n",
    "    plt.ylabel(r'\\textbf{Y}')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure()\n",
    "for vals in np.unique(df['phi'].values):\n",
    "    data = df.query(f'(phi == {vals})')\n",
    "    plt.plot(data['z'], data['r'], '-', alpha=0.5, lw=2)\n",
    "    plt.scatter(data['z'], data['r'], marker='+' )\n",
    "    plt.xlabel(r\"\\textbf{Z}\")\n",
    "    plt.ylabel(r'\\textbf{r}')\n",
    "    plt.grid(True)\n",
    "\"\"\"\n",
    "#########          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring(df_real_tracks, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0.8220720720720721 * (1-0.04450261780104712 -0.002617801047120419)**2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
