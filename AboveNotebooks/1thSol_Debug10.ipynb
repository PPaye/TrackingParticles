{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTitle:       Search_By_Triplet.\\n\\nAutor:       Piter Amador Paye Mamani.\\n\\nDescription:\\n             This python-code is an implementation of the algorithm Search By Triplet that was inspired on the work of\\n             Daniel Campora.           \\nChanges:\\n1.  Squeletum of the algorithm\\n2.  First Tracks. \\n3. \\n4. \\n5.  Adding exceptions. \\n6.  I've deleted unnecessary comments. Also, I was getting an error at time to compute findcandidatewindows.\\n    Problems. One get the values of tracks \\n7.  Changing the jerarquy of the function, according to the paper. It means that findcandidatewindows is calculated\\n    on all modules befero they were processed.\\n6. \\n7. \\n8.  I've added the information of weak_tracks and I've added the information of USED and NOt USED \\n9.  dphi  is a constant value\\n9.  Adding timing information \\n\\n\\n10. In this version I will plot a graphic of efficiency in function of dphi. \\n    Here, I am not concentrating on the plots of the tracks. Only on the plots of the efficiency that depend on dphi.\\n    In other words. I have to run the main program and get the values of the efficiency and then plot. \\n    I am thinking on work only with 0.004 percent of the data. Because it is more fast than all data. \\n        \\n    In another hand, \\n\\n\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from split import *\n",
    "from score import *\n",
    "from scipy import interpolate\n",
    "import time \n",
    "%matplotlib inline \n",
    "import warnings \n",
    "warnings.simplefilter('ignore', np.RankWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Title:       Search_By_Triplet.\n",
    "\n",
    "Autor:       Piter Amador Paye Mamani.\n",
    "\n",
    "Description:\n",
    "             This python-code is an implementation of the algorithm Search By Triplet that was inspired on the work of\n",
    "             Daniel Campora.           \n",
    "Changes:\n",
    "1.  Squeletum of the algorithm\n",
    "2.  First Tracks. \n",
    "3. \n",
    "4. \n",
    "5.  Adding exceptions. \n",
    "6.  I've deleted unnecessary comments. Also, I was getting an error at time to compute findcandidatewindows.\n",
    "    Problems. One get the values of tracks \n",
    "7.  Changing the jerarquy of the function, according to the paper. It means that findcandidatewindows is calculated\n",
    "    on all modules befero they were processed.\n",
    "6. \n",
    "7. \n",
    "8.  I've added the information of weak_tracks and I've added the information of USED and NOt USED \n",
    "9.  dphi  is a constant value\n",
    "9.  Adding timing information \n",
    "\n",
    "\n",
    "10. In this version I will plot a graphic of efficiency in function of dphi. \n",
    "    Here, I am not concentrating on the plots of the tracks. Only on the plots of the efficiency that depend on dphi.\n",
    "    In other words. I have to run the main program and get the values of the efficiency and then plot. \n",
    "    I am thinking on work only with 0.004 percent of the data. Because it is more fast than all data. \n",
    "        \n",
    "    In another hand, \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rho(x,y):\n",
    "    return np.sqrt(x*x + y*y)\n",
    "def r(x,y,z):\n",
    "    return np.sqrt(x*x + y*y + z*z)\n",
    "def theta(x,y,z):\n",
    "    return np.arccos(z/r(x,y,z))\n",
    "def phi(x,y):\n",
    "    return np.arctan(y/x)\n",
    "def module(r):\n",
    "    return np.sqrt(np.sum(r*r))\n",
    "def r_e(z, r_l, r_c):\n",
    "    z_c = r_c[2] \n",
    "    r_versor = (r_l - r_c)/module(r_l - r_c)               # computing r_versor\n",
    "    r_versor_dot_z_versor = r_versor[2]  \n",
    "    return r_c - r_versor/r_versor_dot_z_versor*(z_c - z)  # IMPORTANT WITH THE MINUS SIGN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_data(fraction):\n",
    "    \"\"\"\n",
    "    EVENT \n",
    "    \n",
    "    55microns50psInner55microns50psOuter_EventNumber.txt\n",
    "    \n",
    "    25microns0psInner200microns50psOuter_test.txt\n",
    "    25microns0psInner200microns50psOuter_train.txt\n",
    "    25microns75psInner25microns75psOuter_test.txt\n",
    "    25microns75psInner25microns75psOuter_train.txt\n",
    "    55microns0psInner55microns0psOuter_test.txt\n",
    "    55microns0psInner55microns0psOuter_train.txt\n",
    "    55microns100psInner200microns50psOuter_test.txt\n",
    "    55microns100psInner200microns50psOuter_train.txt\n",
    "    55microns50psInner55microns50psOuter_test.txt\n",
    "    55microns50psInner55microns50psOuter_train.txt\n",
    "    \"\"\"\n",
    "    \n",
    "    name = 'data/25microns75psInner25microns75psOuter_test.txt' # To be modified for others. \n",
    "    data_fraction = fraction\n",
    "    df = pd.DataFrame()\n",
    "    df = pd.read_csv(name, sep=' ')      # All data.\n",
    "    df,_ = Split_frac(df, data_fraction)            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'particle_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/afs/cern.ch/work/p/ppayemam/miniconda/envs/ramp_velo_challenge/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'particle_id'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-84ba119b6787>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DATAFRAME will be a global data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mreading_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-112-55a71ad9570c>\u001b[0m in \u001b[0;36mreading_data\u001b[0;34m(fraction)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# All data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSplit_frac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_fraction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/velo/ramp-velo-challenge-/split.py\u001b[0m in \u001b[0;36mSplit_frac\u001b[0;34m(df, frac)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Maximum number of tracks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'particle_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/work/p/ppayemam/miniconda/envs/ramp_velo_challenge/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/cern.ch/work/p/ppayemam/miniconda/envs/ramp_velo_challenge/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'particle_id'"
     ]
    }
   ],
   "source": [
    "# DATAFRAME will be a global data. \n",
    "fraction = 1\n",
    "df       = reading_data(fraction)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortbyphi(df):\n",
    "    '''Description:\n",
    "    Sort each D_i increasingly accoording to phi\n",
    "    And add a column to the dataframe_module with the name of used to accept or neglect hits. \n",
    "    '''\n",
    "    z = np.sort(df['z'].unique())\n",
    "    df['phi'] = np.arctan(df['x']/df['y'])\n",
    "    modules = [] \n",
    "    for layer_i in z[::1] :\n",
    "        tmp = df.query(f'z=={layer_i}')\n",
    "        tmp = tmp.sort_values('phi', ascending=True)\n",
    "        # IMPORTANT \n",
    "        tmp['used'] = False # MATCHING ALL HITS IN THE MODULES LIKE NOT USED.\n",
    "        modules.append(tmp)  \n",
    "    return modules       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI = []\n",
    "def findcandidatewindows(left_mod, mod, right_mod ):\n",
    "    global phi_extrapolation_coef, phi_extrapolation_base , dphi\n",
    "    '''Description: \n",
    "        Compute the first and last candidates(the window) according to acceptance range(dphi) for each hit.\n",
    "        SUPPOSSING THAT ALL DATA ARE ORDERED ACCOURDING TO PHI. THIS PROCCESS WAS DONE Previously\n",
    "        In case of add more information to the modules, one easily can add throught the iteration \n",
    "    '''\n",
    "    # CONVENTION :     \n",
    "    # l_m  m  r_m   the values are ordered.      \n",
    "    #  |   |   |             \n",
    "    #  |   |   |    phi up  \n",
    "    #  |   |   |    phi      \n",
    "    #  |   |   |    phi down \n",
    "    #  |   |   |          \n",
    "    \n",
    "    right_hit_max = [] \n",
    "    right_hit_min = [] \n",
    "\n",
    "    temporal = mod['phi'] \n",
    "    \n",
    "    # ITERATION OVER PHI FOR RIGHT \n",
    "    \n",
    "    for phi_i in mod['phi']: \n",
    "        if str(phi_i) == 'nan' :     \n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            right_hit_min.append(m) \n",
    "            right_hit_max.append(M) \n",
    "            continue # \n",
    "        if str(phi_i) == 'NaN' :     \n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            left_hit_min.append(m) \n",
    "            left_hit_max.append(M) \n",
    "            continue # \n",
    "            \n",
    "        \n",
    "        z_center = mod['z'].unique()[0]\n",
    "        # GET HIT \n",
    "        \"\"\" dphi =  phi_extrapolation_base + np.abs( z_center ) * phi_extrapolation_coef \"\"\"\n",
    "        \n",
    "        PHI.append(dphi)\n",
    "        \n",
    "        down      = phi_i - dphi \n",
    "        up        = phi_i + dphi \n",
    "        \n",
    "        condition = f'{down} <= phi <=  {up}'\n",
    "        tmp_df = right_mod.query(condition)\n",
    "        if not tmp_df.empty:\n",
    "            m = tmp_df['hit_id'][tmp_df.index[0]]     # minumum hit \n",
    "            M = tmp_df['hit_id'][tmp_df.index[-1]]    # maximum hit \n",
    "            right_hit_min.append(m) \n",
    "            right_hit_max.append(M) \n",
    "        elif tmp_df.empty :\n",
    "\n",
    "            m = \"nan\"                                 # minumum hit \n",
    "            M = \"nan\"                                 # maximum hit\n",
    "            right_hit_min.append(m)  \n",
    "            right_hit_max.append(M) \n",
    "            \n",
    "    left_hit_max = [] \n",
    "    left_hit_min = [] \n",
    "    # ITERATION OVER PHI FOR LEFT\n",
    "    for phi_i in mod['phi']:\n",
    "        if str(phi_i) == 'NaN' :     \n",
    "            \n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            left_hit_min.append(m) \n",
    "            left_hit_max.append(M) \n",
    "            continue # \n",
    "        if str(phi_i) == 'nan' :     \n",
    "            \n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            left_hit_min.append(m) \n",
    "            left_hit_max.append(M) \n",
    "            continue # \n",
    "        \n",
    "        # GET HIT \n",
    "        down      = phi_i - dphi \n",
    "        up        = phi_i + dphi \n",
    "        condition = f'{down} <= phi <= {up}'\n",
    "        tmp_df = left_mod.query(condition)\n",
    "        if not tmp_df.empty :\n",
    "            m = tmp_df['hit_id'][tmp_df.index[0]]        # minumum hit \n",
    "            M = tmp_df['hit_id'][tmp_df.index[-1]]       # maximum hit  \n",
    "            left_hit_min.append(m)\n",
    "            left_hit_max.append(M)\n",
    "        elif tmp_df.empty :\n",
    "            m = \"nan\"               # minumum hit \n",
    "            M = \"nan\"               # maximum hit\n",
    "            left_hit_min.append(m) \n",
    "            left_hit_max.append(M) \n",
    "            \n",
    "    mod[\"right_hit_max\"] = right_hit_max  \n",
    "    mod[\"right_hit_min\"] = right_hit_min  \n",
    "    mod[\"left_hit_max\"]  = left_hit_max   \n",
    "    mod[\"left_hit_min\"]  = left_hit_min   \n",
    "    return mod\n",
    "\n",
    "###############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trackseeding():\n",
    "    global left_mod, mod, right_mod, M_i, dphi\n",
    "    \n",
    "    '''\n",
    "    Description: \n",
    "        Checks the preceding and following modules for compatible hits using the above results.\n",
    "        \n",
    "        All triplets in the search window are fitted and compared.\n",
    "        \n",
    "        and the best one is kept as a track seed.\n",
    "        \n",
    "        stores its best found triplet\n",
    "        Finding triplets is ap- plied in first instance to the modules\n",
    "        that are further apart from the collision point\n",
    "        Each triplet is the seed of a forming track\n",
    "    '''\n",
    "\n",
    "    #Necessary functions.\n",
    "    def fit(triplet): \n",
    "        phi_data = [ df.query(f'hit_id == {hit}')['phi'] for hit in triplet ]\n",
    "        z_data   = [ df.query(f'hit_id == {hit}')['z']   for hit in triplet ]\n",
    "        phi_data = [ hit.values[0] for hit in phi_data                      ]                        \n",
    "        z_data   = [ hit.values[0] for hit in z_data                        ]                    \n",
    "        # Kind of fit: Linear\n",
    "        fitting = np.polyfit(phi_data, z_data, 1)\n",
    "        chiSquared = np.sum((np.polyval(fitting, z_data) - phi_data)**2)\n",
    "        return chiSquared\n",
    "\n",
    "    df_triplets = []\n",
    "\n",
    "    for index, part in mod.iterrows():\n",
    "\n",
    "        r_hit_max, r_hit_min = part[\"right_hit_max\"], part[\"right_hit_min\"]  \n",
    "        l_hit_max, l_hit_min = part[\"left_hit_max\"],  part[\"left_hit_min\" ] \n",
    "        \n",
    "        if  str(r_hit_max)  == \"nan\":\n",
    "            continue \n",
    "        elif str(r_hit_min) == \"nan\":\n",
    "            continue \n",
    "        elif str(l_hit_max) == \"nan\":\n",
    "            continue \n",
    "        elif str(l_hit_min) == \"nan\":\n",
    "            continue  \n",
    "        if  str(r_hit_max)  == \"NaN\" :\n",
    "            continue \n",
    "        elif str(r_hit_min) == \"NaN\" :\n",
    "            continue \n",
    "        elif str(l_hit_max) == \"NaN\" :\n",
    "            continue \n",
    "        elif str(l_hit_min) == \"NaN\" :\n",
    "            continue  \n",
    "        \n",
    "        r_phi_max = right_mod.query(f\"hit_id == {r_hit_max}\")['phi'].values[0]\n",
    "        r_phi_min = right_mod.query(f\"hit_id == {r_hit_min}\")['phi'].values[0] \n",
    "        \n",
    "        l_phi_max = left_mod.query(f\"hit_id == {l_hit_max}\")['phi'].values[0]  \n",
    "        l_phi_min = left_mod.query(f\"hit_id == {l_hit_min}\")['phi'].values[0]  \n",
    "        \n",
    "        left_mod.query(f\" {l_phi_min} <= phi <= {l_phi_max}\")\n",
    "        \n",
    "        #Forming all Triplets. \n",
    "        # Here I am adding the condition of used and not used.\n",
    "        # After of this I need to change the condition of used and not used. \n",
    "        \n",
    "        tmp_right = right_mod.query(f\"   {r_phi_min} <= phi <= {r_phi_max} & used == False  \")    # ADDING TIME\n",
    "        for R in tmp_right['phi'].values:\n",
    "            tmp_left = left_mod.query(f\" {l_phi_min} <= phi <= {l_phi_max} & used == False \")     # ADDING TIME\n",
    "            for L in tmp_left['phi'].values: \n",
    "                \n",
    "                hit_left   = int( tmp_left.query( f\" phi == {L}\")['hit_id'].values[0]  )  \n",
    "                hit_center = int( part[\"hit_id\"] )\n",
    "                hit_right  = int( tmp_right.query(f\" phi == {R}\")['hit_id'].values[0]  )                \n",
    "                # With this data we have built the triplets. \n",
    "                triplets = [hit_left, hit_center, hit_right]\n",
    "                # This a lost of memory. I mean that call by hits and not by values is a lost of memory.\n",
    "                chi2 = fit(triplets)                                                                                                                                                                \n",
    "                # Finally we append the values of the data to a df_triplets\n",
    "                df_triplets.append(list(triplets)+[chi2])\n",
    "                \n",
    "    df_triplets = pd.DataFrame(df_triplets, columns = ['left_hit', 'hit', 'right_hit', 'chi2'])  \n",
    "    # Up to this point it is necessary to have the values of df_triplets complete\n",
    "    # Then the algorithm should continue to get the best choices according to the values\n",
    "    # of chi2.\n",
    "    def best_choice(df_triplets):\n",
    "        seeds = []\n",
    "        for hit_c in df_triplets['hit'].unique() : # UNIQUE\n",
    "            # GROUPING \n",
    "            tmp = df_triplets.query(f'hit == {hit_c}')\n",
    "            minimum = (tmp['chi2']).idxmin()\n",
    "            t = (tmp.loc[minimum]).values            \n",
    "            t = [int(i) for i in t]\n",
    "            #these are the triplets       \n",
    "            seeds.append(list(t[:3]))     # Here I am negleting the information chi2 because is not important\n",
    "        return seeds                      # obviously it is a track\n",
    "\n",
    "    seeds = best_choice(df_triplets)\n",
    "\n",
    "    \n",
    "    \n",
    "    for seed in seeds:\n",
    "            # #########     MARKING TRIPLES######      \n",
    "            hit_id_left, hit_id_center, hit_id_right = seed\n",
    "            #LEFT\n",
    "            left_mod.loc[   left_mod.hit_id == hit_id_left,    \"used\" ]     = True\n",
    "            #CENTER\n",
    "            mod.loc[           mod.hit_id   == hit_id_center,  \"used\" ]     = True\n",
    "            #RIGTH\n",
    "            right_mod.loc[ right_mod.hit_id == hit_id_right,   \"used\" ]     = True\n",
    "    return seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACKS = []\n",
    "def track_forwarding():\n",
    "    # Here I have to review the leff_mod\n",
    "    # So, I am beging to print the value of left_mod on three steps \n",
    "    global tracks, work_module, left_mod, mod, right_mod, M_i, weak_tracks \n",
    "    # for \n",
    "    global phi_extrapolation_coef, phi_extrapolation_base \n",
    "\n",
    "    global TRACKS\n",
    "    \n",
    "    new_tracks = [] \n",
    "    # Notation:\n",
    "    # x0, y0, z0 is the EXTRAPOLATED track.               \n",
    "    # X,  Y,  Z  is the last track on previous module.   \n",
    "    # x,  y,  z  is the tracks on a window.                                                                 \n",
    "    # Searching tracks on phi_e - dphi < phi < phi_e + dphi that minimize the extrapolated function.\n",
    "    # r0 = np.array([x0, y0, z0] )\n",
    "    # r  = np.array([x, y, 1] )\n",
    "    # R  = np.array([X,  Y,  Z ] )\n",
    "    def module(r):\n",
    "        return np.sqrt(np.sum(r*r))\n",
    "    def ext_func(r0, r1, r):\n",
    "        # r0, r1, r are arrays\n",
    "        dx2_plus_dy2 = module(  r0-r )     # distance between hits on the working module.  \n",
    "        dz2          = module( r1-r0 )     # distance between the last two modules.                                  \n",
    "        return dx2_plus_dy2/dz2 \n",
    "    \n",
    "    z_e = float(work_module['z'].unique()) #z_position of work_module                    \n",
    "    \n",
    "    # here the track is exactly the seed. Only for the 1th iteration\n",
    "    for track in tracks: \n",
    "        #PROOF: Do you have the track values information of USED ?\n",
    "        data = []          \n",
    "        vector_data = []  \n",
    "        \n",
    "        #EXTRAPOLATION ONLY WITH TWO LAST HITS\n",
    "        \n",
    "        for hit in track[0:2] :\n",
    "            data.append(tuple((df.query(f'hit_id == {hit}')[['phi', 'z']]).values[0]))     \n",
    "            vector_data.append(tuple((df.query(f'hit_id == {hit}')[['x', 'y', 'z']]).values[0]))\n",
    "        phi_data, z_data = zip(*data) \n",
    "        #EXTRAPOLATED SEGMENT FUNCTION      \n",
    "        ext_seg = interpolate.interp1d(z_data, phi_data, fill_value = \"extrapolate\" )\n",
    "        phi_e   = ext_seg(z_e) \n",
    "        r_l, r_c = vector_data                   # THE VALUES ON LEFT AND RIGHT                                                \n",
    "        r_l, r_c = np.array(r_l), np.array(r_c)  # \n",
    "        x_e, y_e, z_e = r_e(z_e, r_l, r_c)       # COMPUTING THE VALUES ON THE WORKING MODULE.                                 \n",
    "        \n",
    "        #Open a Window centered on phi_e: \n",
    "        z_center = mod['z'].unique()[0]\n",
    "        #z = df.query(f\"phi=={phi_i}\")[\"z\"].values[0]\n",
    "        # GET HIT \n",
    "        \"\"\"dphi =  phi_extrapolation_base + np.abs( z_center ) * phi_extrapolation_coef \"\"\"\n",
    "        \n",
    "        down = phi_e - dphi\n",
    "        up   = phi_e + dphi   \n",
    "\n",
    "        if str(down) == 'nan' or str(down) == 'NaN' or str(up) == 'nan' or str(up) == 'NaN' :\n",
    "            print(\"An error ocurred with the values of down or up. Plese cheack.\")\n",
    "            break\n",
    "        \n",
    "        #################################### WINDOW ########################### \n",
    "        df_work_module_window = work_module.query(f\" {down} <= phi <= {up} \" )  \n",
    "        #print(\"df_work_module_window\")\n",
    "        #print(df_work_module_window.head())\n",
    "        \n",
    "        #This definition will be done after the loop. \n",
    "        #df_candidates = pd.DataFrame(columns=[\"hit_id\", \"ext_fun\"]) # This dataframe have to be reviwed \n",
    "        hit_left = track[0]   \n",
    "        R  = df.query(f'hit_id == {hit_left}')[['x','y','z']].values[0] # this value would have to change on each track\n",
    "        r0 = np.array([x_e, y_e ,z_e ])\n",
    "\n",
    "        tmp_candidates = []\n",
    "        for index, row in df_work_module_window.iterrows(): \n",
    "            # Here I only need to have the information of position.\n",
    "            r      =  row[['x', 'y', 'z']].values # this value is variable on each row.\n",
    "            hit_id =  row['hit_id']    \n",
    "            ext_func_value = ext_func(r0, R, r)\n",
    "            tmp_candidates.append( [hit_id, ext_func_value] )\n",
    "        \n",
    "        #\"************************************************************************************************************\" \n",
    "        #\"************************************If any extrapolated data is not founded on the working module **********\" \n",
    "        #\"************************************************************************************************************\"\n",
    "        #\"**********************************HERE WE ARE LOSING TRACKS***********************************************\"\n",
    "        #\"************************************************************************************************************\"\n",
    "        #\"************************************************************************************************************\"\n",
    "        #\"************************************************************************************************************\"\n",
    "        #\"************************************************************************************************************\"\n",
    "        #\"************************************************************************************************************\"\n",
    "        #\"************************************************************************************************************\"\n",
    "\n",
    "        # this part of the code makes it inefficient\n",
    "        if tmp_candidates == [] :                                             \n",
    "            # In case of don't find a hit on the working module.  \n",
    "            # there are two cases\n",
    "            # = = \n",
    "            \n",
    "            # = = \n",
    "            hit_id_left, hit_id_center, hit_id_right = track[0:3]\n",
    "            \n",
    "            # print(len(track))\n",
    "            # print(track)\n",
    "            # print( (hit_id_left   in       mod['hit_id'].values ) )\n",
    "            # print( (hit_id_center in       mod['hit_id'].values ) )\n",
    "            # print( (hit_id_right  in right_mod['hit_id'].values ) )\n",
    "            \n",
    "            TRACKS.append(track)\n",
    "            # the track has its first forwarding \n",
    "            if ( (hit_id_left in left_mod['hit_id'].values ) ):# and  (hit_id_center in mod['hit_id'].values) and (hit_id_right in right_mod['hit_id'].values ) ) : \n",
    "                same_track     = [] + track  \n",
    "                new_tracks.append(same_track)\n",
    "                continue\n",
    "            # the track has its second worwarding    \n",
    "            elif ( (hit_id_left in mod['hit_id'].values )  ): #and  (hit_center in right_mod['hit_id'].values) ):\n",
    "                weak_tracks.append(track)\n",
    "                # Add to weak_tracks                \n",
    "                continue\n",
    "            else :\n",
    "                continue\n",
    "        # ¿ ******************************************************************************************************** ? \" \n",
    "        # \"************************************************************************************************************\" \n",
    "        # \"************************************tmp_candidates**********************************************************\" \n",
    "        # \"************************************************************************************************************\"\n",
    "                \n",
    "        df_candidates = pd.DataFrame(tmp_candidates, columns=[\"hit_id\", \"ext_fun\"])\n",
    "        \n",
    "        if len(tmp_candidates) == 0 :\n",
    "            print(\"an error ocurred with df_candidates\")\n",
    "            break\n",
    "        \n",
    "        # Choosing new hit_id to complete the track.  \n",
    "        new_hit_id    = df_candidates.loc[df_candidates['ext_fun'].idxmin()]['hit_id']\n",
    "        new_hit_id    = int(new_hit_id)   # new_hit_id   \n",
    "    \n",
    "        # MARKING EACH HIT AS USED ON THE WORKING MODULE        \n",
    "        work_module.loc[ work_module.hit_id == new_hit_id, \"used\" ]  = True   \n",
    "        \n",
    "        new_track     = [new_hit_id] + track  \n",
    "        new_tracks.append(new_track)\n",
    "         \n",
    "    return new_tracks  # this value will be replaced by tracks on the main algorithm\n",
    "\"\"\"\n",
    "#delete\n",
    "append to weaktracks              \n",
    "weak_tracks.append(track)        \n",
    "# MATCHING EACH HIT AS USED ON THE WORKING MODULE  \n",
    "hit_id_left, hit_id_center, hit_id_right = track[0:3] \n",
    "#LEFT\n",
    "left_mod.loc[left_mod.hit_id == hit_id_left, \"used\"]     = True\n",
    "#CENTER\n",
    "mod.loc[        mod.hit_id == hit_id_center, \"used\"]     = True\n",
    "#RIGTH\n",
    "right_mod.loc[right_mod.hit_id == hit_id_right, \"used\"]  = True\n",
    "# I CAN MARK LIKE USED\n",
    "        if len(track) == 3 :\n",
    "            # #########     MARKING TRIPLES######  \n",
    "            # MATCHING EACH HIT AS USED ON THE WORKING MODULE  \n",
    "\n",
    "            hit_id_left, hit_id_center, hit_id_right = track\n",
    "\n",
    "            #LEFT\n",
    "            left_mod.loc[left_mod.hit_id == hit_id_left, \"used\"]     = True\n",
    "            #CENTER\n",
    "            mod.loc[        mod.hit_id == hit_id_center, \"used\"]     = True\n",
    "            #RIGTH\n",
    "            right_mod.loc[right_mod.hit_id == hit_id_right, \"used\"]  = True\n",
    "\n",
    "            # FINALLY we add the new track to the value of tracks\n",
    "            # ADDING to track\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "################################## MAIN ################################################\n",
    "########################################################################################\n",
    "############################ GENERAL ALGORITHM #########################################\n",
    "############################     PARAMETERS    #########################################\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "#########################      GENERAL LOOP       ######################################\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "######################### DPHI HAVE A MULTIPLE VALUES ##################################\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "\n",
    "T1 = time.time() \n",
    "\n",
    "efficiency = []\n",
    "fake       = []\n",
    "ghost_rate = []\n",
    "\n",
    "df_real_tracks = df.groupby(['particle_id'])['hit_id'].unique() # this is a series.\n",
    "\n",
    "for dphi in np.linspace(0.01, 0.1, 10): \n",
    "    print(\"================================================================================================\")\n",
    "    print(\"the value of dphi is :\", dphi)\n",
    "    \n",
    "    fraction = 1 # \n",
    "    \n",
    "    #this line will have to be commented\n",
    "    #dphi     = 0.001  # The windows is a variable quantity that depend on phi_ext_base ###### \n",
    "    # phi_window     =  phi_extrapolation_base + np.abs( hit_Zs[h_center]) * phi_extrapolation_coef         ;\n",
    "\n",
    "    phi_extrapolation_coef = 0.02\n",
    "    phi_extrapolation_base = 0.03   \n",
    "                                         #m = 24    # number of modules counted from the left.  #\n",
    "    m = 23                               # from 1 to 24. No more. \n",
    "\n",
    "    new_tracks = []                      # \n",
    "    df = reading_data(fraction)          # where data is unmodified.\n",
    "    # df_search   = df_original          # where I am searching \n",
    "    tracks = []                          # [[1,24, 5], [7,6,4] ,[346,7,32,], ... ]\n",
    "    weak_tracks  =[]                     # 123\n",
    "    # *********************IMPORTANT**********************************************\n",
    "    # The information of tracks is ordered. \n",
    "    # Because each of its elements are an ordered list according to module layers.\n",
    "    # However, the information of hits are unique and not matter if are a ordered set. \n",
    "    # But it was filled out in order\n",
    "\n",
    "    # SEPARATION BY MODULE  \n",
    "    modules = sortbyphi(df)  \n",
    "    # FIND CANDIDATE WINDOWS. In order to minimize the amount of candidates considered in subqsequent steps.\n",
    "\n",
    "    for M_i in range(len(modules)-1, len(modules)-m-1, -1): \n",
    "        M_i = M_i - 1\n",
    "        left_mod     =  modules[M_i - 1]   \n",
    "        mod          =  modules[M_i    ]  \n",
    "        right_mod    =  modules[M_i + 1]  \n",
    "        modules[M_i] =  findcandidatewindows(left_mod, mod, right_mod)\n",
    "\n",
    "    #ITERATION OVER MODULES ( ):\n",
    "\n",
    "    for M_i in range(len(modules)-1-1, len(modules)-m-2, -1):  # the number two is due to 1. index postion default. 2. \n",
    "        # print(M_i)\n",
    "    #for M_i in range(22, 20, -1):\n",
    "\n",
    "        # TIMING THE RUNNING OVER A MODULE\n",
    "\n",
    "        t1 = time.time()                \n",
    "        print(f\"module number {M_i}\")\n",
    "        #M_i = M_i - 1\n",
    "\n",
    "        #1th STEP:  ASIGNING NOTATION\n",
    "        left_mod  =  modules[M_i - 1]  \n",
    "        mod       =  modules[M_i    ]  \n",
    "        right_mod =  modules[M_i + 1] \n",
    "\n",
    "        new_seeds  = trackseeding()     \n",
    "        #Adding new seeds to tracks \n",
    "        tracks = tracks + new_seeds\n",
    "\n",
    "        # REASIGNING VALUES    \n",
    "        modules[M_i - 1] = left_mod           \n",
    "        modules[M_i    ] = mod              \n",
    "        modules[M_i + 1] = right_mod        \n",
    "\n",
    "        # Defining a new module.  \n",
    "        work_module = modules[M_i - 2] \n",
    "\n",
    "        #print(left_mod['used'], mod['used'], right_mod['used'])\n",
    "\n",
    "        new_tracks  = track_forwarding() \n",
    "        tracks = new_tracks\n",
    "\n",
    "        #Reasigning \n",
    "        modules[M_i - 2] = work_module\n",
    "        #\n",
    "        t2 = time.time()\n",
    "        print(\"time per module\", t2-t1) \n",
    "    tracks = pd.Series(new_tracks)\n",
    "\n",
    "    d =  Scoring(df_real_tracks, tracks)    \n",
    "    e, f, c = d.values()\n",
    "    \n",
    "    efficiency.append(e)\n",
    "    fake.append(f)\n",
    "    ghost_rate.append(c)\n",
    "    \n",
    "print(\"FINDING TRACKS FINISHED\")\n",
    "T2 = time.time() \n",
    "######################\n",
    "######  FINALLY  #####\n",
    "###### COMPARING #####\n",
    "######################\n",
    "print(\"TOTAL TIME IS : \", T2-T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fake, efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ghost_rate, efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in np.sort(df['z'].unique()): \n",
    "    # print the number of particles\n",
    "    # print(z)\n",
    "    df.query(f\"z == {z}\").hist('t')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
